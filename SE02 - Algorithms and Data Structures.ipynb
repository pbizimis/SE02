{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SE02 Algorithms and Data Structures Portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This portfolio was created by Philip Bizimis. The main sources for the content were the books *The Algorithm Design Manual* by Steven S. Skiena, *Grokking Algorithms: An Illustrated Guide for Programmers and Other Curious People* by Aditya Bhargava, the Khanacademy course on Algorithms and the internet (more detailed sources list at the end). <br>\n",
    "\n",
    "This portfolio was created for the SE02 assessment but I try to build portfolios in a way that they serve me after the assessment as well. Thus, I tried to avoid long text paragraphs to give myself the chance to quickly find the notes I was looking for. <br>\n",
    "\n",
    "The last edit for the assessment version happened on the 13.07.2020. The pdf version will be accompanied by the same version on GitHub to interact with links. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. Algorithm Theory\n",
    "\n",
    "    1.1 Asymptotic Analysis  \n",
    "\n",
    "    1.2 Asymptotic Notation  \n",
    "\n",
    "    1.3 Algorithm Design Strategies  \n",
    "    \n",
    "\n",
    "2. Algorithm Examples  \n",
    "\n",
    "    2.1 Searching  \n",
    "\n",
    "    2.2 Sorting  \n",
    "\n",
    "    2.3 Recursive Algorithms  \n",
    "\n",
    "    2.4 Divide and Conquer  \n",
    "\n",
    "    2.5 Breadth-First Search\n",
    "\n",
    "    2.6 Depth-First Search\n",
    "\n",
    "    2.7 Greedy Algorithms\n",
    "\n",
    "    2.8 A* Algorithm\n",
    "\n",
    "    2.9 Nearest Neighbour Search Algorithm  \n",
    "    \n",
    "\n",
    "3. Data Structures\n",
    "\n",
    "    3.1 Linked List\n",
    "\n",
    "    3.2 Array\n",
    "\n",
    "    3.3 Stack\n",
    "\n",
    "    3.4 Queue\n",
    "\n",
    "    3.5 Trees\n",
    "\n",
    "    3.6 Priority Queue\n",
    "\n",
    "    3.7 Dictionaries\n",
    "\n",
    "    3.8 Graphs  \n",
    "    \n",
    "\n",
    "4. Investigating a Technology  \n",
    "\n",
    "\n",
    "5. Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Algorithm Theory\n",
    ">algorithm (noun):\n",
    "\"A process or set of rules to be followed in calculations or other problem-solving operations, especially by a computer.\" (Oxford Languages)\n",
    "\n",
    "Algorithms are a part of us. We use them every day and probably do not even realize it. An algorithm can be as simple as: If I come home, I wash my hands and turn the lights on.  \n",
    "Nevertheless, the real power of algorithms comes in form of computer programs. There are many different applications like sorting, searching or compressing. Furthermore, there are many algorithms that, in the end, solve the same problem. To compare theses algorithms they have to be analyzed. This analyzation of an algorithm is called asymptotic analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Asymptotic Analysis\n",
    "Asymptotic analysis of an algorithm refers to defining the mathematical boundation/framing of its run-time performance. Using asymptotic analysis, we can very well conclude the best case, average case, and worst case scenario of an algorithm.\n",
    "\n",
    "There are rules to follow in order to express the correct asymptotic analysis of an algorithm:\n",
    "- As the analysis is asymptotic, it refers to large inputs (n). This means that the terms that grow \"most quickly\" will make the other ones irrelavant.\n",
    "    - If the function is a sum of several terms, the one with the largest growth rate can be kept and the others omitted.\n",
    "    - If the function is a product of several factors, any constants can be omitted.\n",
    "\n",
    "#### Time Complexity\n",
    "\n",
    "The time complexity of an algorithm relates the length of an algorithm's input to the number of steps it takes.\n",
    "\n",
    "We do not want to give the running time of an algorithm in a time unit because this would mean that it is only comparable with the same implementations (programming language, compiler, hardware, etc.). Thus, it is given as a function:\n",
    "\n",
    "The RAM (random access machine) Model of Computation:\n",
    "- this is the hypothetical computer in order to have a language- and machine-independent analysis \n",
    "    - basic logical or arithmetic operations (+,\\*,=,if,call) are considered to be simple operations that take one time step\n",
    "    - loops and subroutines are complex operations composed of multiple time steps based on the number of iterations\n",
    "    - memory access takes one time step\n",
    "\n",
    "#### Space Complexity\n",
    "\n",
    "The space complexity of an algorithm relates the length of an algorithm's input to the number of storage locations it uses.\n",
    "\n",
    "It is also expressed asymptotically. Space complexity is a measure of the amount of working storage an algorithm needs. This amount is, again, dependent on the input size.\n",
    "Auxiliary space is not the same as space complexity even though they are sometimes (wrongly) used for the same thing. Space complexity is the sum of the auxiliary space and the input space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Simple Example\n",
    "```python\n",
    "def print_items(array):\n",
    "    for item in array:\n",
    "        print(item)\n",
    "    return True\n",
    "```\n",
    "- the function runs through every item in the list once\n",
    "- for an array with length n it runs n times\n",
    "- constant terms (e.g. returning or printing some value) are ignored\n",
    "    - (constant terms can make a difference if the compared algorithms have the same running time)\n",
    "- time complexity of $\\Theta(n)$\n",
    "\n",
    "\n",
    "- space complexity\n",
    "    - the array takes n units of space, as the length can vary\n",
    "    - the item variable is constant as the new item will be saved and the old item will be discarded\n",
    "    - thus, we have a space complexity of $\\Theta(n)$\n",
    "    - if we talk about auxiliary space, the input array would not be counted\n",
    "        - thus, we have a auxiliary space complexity of $\\Theta(1)$ <br>\n",
    "        \n",
    "*(This is a very good example why the term space complexity cannot be used for auxiliary space complexity)*  \n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Asymptotic Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Big-O Notation\n",
    "\"the running time grows at most this much, but it could grow more slowly\"\n",
    "- expresses the upper bound\n",
    "\n",
    "<img style=\"width: 300px;\" src=\"https://cdn.programiz.com/sites/tutorial2program/files/big0.png\">  \n",
    "\n",
    "<p style=\"text-align: center;\"><a href=\"https://cdn.programiz.com/sites/tutorial2program/files/big0.png\">Source</a></p>\n",
    "\n",
    "For a function $g(n)$, $O(g(n))$ is given by the relation:\n",
    "\n",
    "```\n",
    "O(g(n)) = { f(n): there exist positive constants c and n0\n",
    "            such that 0 ≤ f(n) ≤ cg(n) for all n ≥ n0 }\n",
    "\n",
    "```\n",
    "*Note to self: Many use Big-O notation even if Big-Theta would be more fitting.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Big-$\\Theta$ Notation\n",
    "\"the function grows asymptotically as fast as ,e.g., $n^2$\"\n",
    "- expresses the upper bound and lower bound (tight bound)\n",
    "\n",
    "<img style=\"width: 300px;\" src=\"https://cdn.programiz.com/sites/tutorial2program/files/theta.png\">\n",
    "<p style=\"text-align: center;\"><a href=\"https://cdn.programiz.com/sites/tutorial2program/files/theta.png\">Source</a></p>\n",
    "\n",
    "For a function $g(n)$, $\\Theta(g(n))$ is given by the relation:\n",
    "\n",
    "```\n",
    "Θ(g(n)) = { f(n): there exist positive constants c1, c2 and n0\n",
    "            such that 0 ≤ c1g(n) ≤ f(n) ≤ c2g(n) for all n ≥ n0 }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Big-$\\Omega$ Notation:\n",
    "\"the running time grows at least this much\"\n",
    "- expresses the lower bound\n",
    "\n",
    "<img style=\"width: 300px;\" src=\"https://cdn.programiz.com/sites/tutorial2program/files/omega.png\">\n",
    "<p style=\"text-align: center;\"><a href=\"https://cdn.programiz.com/sites/tutorial2program/files/omega.png\">Source</a></p>\n",
    "\n",
    "For a function $g(n)$, $\\Omega(g(n))$ is given by the relation:\n",
    "\n",
    "```\n",
    "Ω(g(n)) = { f(n): there exist positive constants c and n0 \n",
    "            such that 0 ≤ cg(n) ≤ f(n) for all n ≥ n0 }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best, Worst, Average Case\n",
    "Important to note is that for each time complexity there can be a best, worst and average case. To clarify this, I want to use this example:\n",
    "```\n",
    "Contrive(n)\n",
    "if n = 0 then do something Theta(n^3)\n",
    "else if n is even then\n",
    "    flip a coin\n",
    "    if heads, do something Theta(n)\n",
    "    else if tails, do something Theta(n^2)\n",
    "else if n is odd then\n",
    "    flip a coin\n",
    "    if heads, do something Theta(n^4)\n",
    "    else if tails, do something Theta(n^5)\n",
    "\n",
    "```\n",
    "If this would be an algorithm, we can find different asymptotic behavior of this function. <br>\n",
    "In the best case (n is even), the runtime is $\\Omega(n)$ and $O(n^2)$ but not $\\Theta$ of anything. <br>\n",
    "In the worst case (n is odd), the runtime is $\\Omega(n^4)$ and $O(n^5)$ but not $\\Theta$ of anything. <br>\n",
    "In the case n = 0, the running time is $\\Theta(n^3)$.  \n",
    "[Source](https://cs.stackexchange.com/questions/23068/how-do-o-and-%ce%a9-relate-to-worst-and-best-case)\n",
    "\n",
    "This also means that the instances of the size n define each case. In the example above, it is odd, even or 0. For some sorting algorithms, it can be that n is already sorted, almost sorted or sorted in reverse. For searching algorithms this may be that the looked for element is at index 0, at index n-1 or not in the input at all.\n",
    "Concrete examples will be given when algorithms are analyzed.\n",
    "\n",
    "#### Adding Functions\n",
    "- the sum of two functions is governed by the dominant one <br>\n",
    "$O(f(n)) + O(g(n)) → O(max(f(n),g(n)))$<br>\n",
    "$Ω(f(n)) + Ω(g(n)) → Ω(max(f(n),g(n)))$ <br>\n",
    "$Θ(f(n)) + Θ(g(n)) → Θ(max(f(n),g(n)))$ <br>\n",
    "\n",
    "#### Multiplying Functions\n",
    "- as described above, constants do not affect the asymptotic behavior of a function and thus, can be omitted  \n",
    "$O(c · f(n)) → O(f(n))$ <br>\n",
    "$Ω(c · f(n)) → Ω(f(n))$ <br>\n",
    "$Θ(c · f(n)) → Θ(f(n))$ <br>\n",
    "\n",
    "\n",
    "- two functions in a product are important as both increase  \n",
    "$O(f(n)) ∗ O(g(n)) → O(f(n) ∗ g(n))$ <br>\n",
    "$Ω(f(n)) ∗ Ω(g(n)) → Ω(f(n) ∗ g(n))$ <br>\n",
    "$Θ(f(n)) ∗ Θ(g(n)) → Θ(f(n) ∗ g(n))$ <br>\n",
    "\n",
    "#### Example Terms\n",
    "\n",
    "- $O(1)$\t    constant\n",
    "- $O(log n)$\tlogarithmic\n",
    "- $O(n)$\t    linear\n",
    "- $O(n log n)$\tn log n\n",
    "- $O(n^2)$\t    quadratic\n",
    "- $O(n^3)$\t    cubic\n",
    "- $n^{O(1)}$\t    polynomial\n",
    "- $2^{O(n)}$\t    exponential\n",
    "\n",
    "#### Properties of Logarithm\n",
    "We know the formula to change the base of a logarithm: <br>\n",
    "$log_ab = {log_c b}/{log_ca}$\n",
    "\n",
    "This is important for asymptotic notation because, as the formula shows, it is easy to change from base-a to base-c by dividing by $log_ca$. <br>\n",
    "This conversion factor is lost to asymptotic notation whenever a and c are constants. Thus, the base of the logarithm can be omitted when analyzing algorithms.\n",
    "\n",
    "#### Dominance Pecking Order\n",
    "n! >> c^n >> n^3 >> n^2 >> n^{1+ε} >> n log n >> n >> √n >> log^2n >> log n >> log n/ log log n >> log log n >> α(n) >> 1  \n",
    "\n",
    "#### Same Time Complexity\n",
    "The last thing I would like to mention is how we compare two algorithms with the same time complexity.\n",
    "To do that, I want to use this quote:\n",
    "> *But how can we compare two Θ(n log n) algorithms to decide which is faster?\n",
    "How can we prove that quicksort is really quick? Unfortunately, the RAM model\n",
    "and Big Oh analysis provide too coarse a set of tools to make that type of distinction. When faced with algorithms of the same asymptotic complexity, implementation details and system quirks such as cache performance and memory size may\n",
    "well prove to be the decisive factor.*\n",
    "> - Skiena, Steven S. 1998. The Algorithm Design Manual (page 129)  \n",
    "\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Algorithm Design Strategies\n",
    "\n",
    "#### Brute Force Algorithms\n",
    "- straightforward methods of solving a problem that rely on computing power and trying every possibility rather then advanced techniques to improve efficiency\n",
    "\n",
    "#### Backtracking Algorithms\n",
    "- systematic way to iterate through all the possible configurations of a search space\n",
    "- if a part solution seems to not lead to a full solution, steps are taken back to find alternative part solutions\n",
    "- either a solution is found or it can be said that there is none\n",
    "- usually these algorithms are implemented recursively\n",
    "- ensure correctness by enumerting all possibilities\n",
    "- ensure efficiency by never visiting a state more than once\n",
    "\n",
    "#### Dynamic Programming\n",
    "- useful when you’re trying to optimize something given a constraint (maximizing or minimizing a function)\n",
    "- combines greedy algorithms that make the best local decision at each step and exhaustive search algorithms that try all possibilities and select the best always to produce the optimum result\n",
    "    - systematically search all possibilites while storing results to avoid recomputing\n",
    "        - efficiency and correctness\n",
    "- every dynamic-programming solution involves a grid\n",
    "- the values in the cells are usually what you’re trying to optimize\n",
    "- each cell is a subproblem: How can you divide your problem into subproblems?\n",
    "- recursive subproblems that are the same over and over can look up the once computed value in the table\n",
    "- optimal substructure (the optimal solution can be created from optimal solutions of its subproblems)\n",
    "- overlapping subproblems (supproblem is solved multiple times)\n",
    "- there is no single formula for calculating a dynamic-programming solution\n",
    "\n",
    "#### Greedy Algorithms\n",
    "- at each step one picks the locally optimal move\n",
    "- result may not be optimal (local optimum which can be a global optimum)\n",
    "- \"approximate\" the optimal solution\n",
    "\n",
    "#### Parallel Algorithms\n",
    "- parallel processing is more and more important with cloud computing, cluster computing and multicore processors\n",
    "- example\n",
    "    - high-resolution graphics applications must render a lot of frames per second for realistic animations\n",
    "    - dividing the image into regions and assigning those to different processors might be the only solution to achieve that\n",
    "\n",
    "#### Divide and Conquer\n",
    "- based on recursion\n",
    "- breaks a problem into subproblems that are similar to the original problem\n",
    "\n",
    "\n",
    "1. Divide the problem into a number of subproblems that are smaller instances of the same problem.\n",
    "2. Conquer the subproblems by solving them recursively. If they are small enough, solve the subproblems as base cases.\n",
    "3. Combine the solutions to the subproblems into the solution for the original problem.  \n",
    "\n",
    "The [master theorem](https://www.programiz.com/dsa/master-theorem):  \n",
    "\n",
    "The master method is a formula for solving recurrence relations of the form: <br>\n",
    "$T(n) = aT(n/b) + f(n)$, <br>\n",
    "where, <br>\n",
    "n = size of input <br>\n",
    "a = number of subproblems in the recursion <br>\n",
    "n/b = size of each subproblem. All subproblems are assumed \n",
    "     to have the same size. <br>\n",
    "f(n) = cost of the work done outside the recursive call, \n",
    "      which includes the cost of dividing the problem and\n",
    "      cost of merging the solutions\n",
    "\n",
    "Here, a ≥ 1 and b > 1 are constants, and f(n) is an asymptotically positive function.\n",
    "\n",
    "If this is the case, the time complexity of a recursive relation is given by:\n",
    "\n",
    "$T(n) = aT(n/b) + f(n)$\n",
    "\n",
    "where, T(n) has the following asymptotic bounds:  \n",
    "\n",
    "1. If f(n) = $O(n^{log_b a-ϵ})$, then T(n) = $Θ(n^{log_b a})$\n",
    "    - subproblems overshadow work to split/recombine a problem\n",
    "    - the recursion tree (solution tree) is leaf-heavy  \n",
    "\n",
    "\n",
    "\n",
    "2. If f(n) = $Θ(n^{log_b a})$, then T(n) = $Θ(n^{log_b a} *log n)$\n",
    "    - work to split/recombine a problem is comparable to subproblems  \n",
    "\n",
    "\n",
    "\n",
    "3. If f(n) = $Ω(n^{log_b a+ϵ})$, then T(n) = $Θ(f(n))$\n",
    "    - work to split/recombine a problem dominates subproblems\n",
    "    - the recursion tree (solution tree) is root-heavy  \n",
    "\n",
    "\n",
    "\n",
    "ϵ > 0 is a constant.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Where does this formula come from? The mathematical proof can be found [here](https://www.cs.cornell.edu/courses/cs3110/2012sp/lectures/lec20-master/mm-proof.pdf). More suiting to this portfolio is the intuition why this applies. \n",
    "A divide and conquer algorithm divides the problem into a number of subproblems. Using the variables of the formula, each subproblem is assumed to be size n/b. If we think about the solution as a tree, we get a solution tree where each node is a recursive call and the children of that node are recursive calls from that call. The leaves of the tree, as we learn in the data structures chapter, do not have children and thus, are the base cases. Each node, except of the leaf-nodes, would have $a$ child nodes. A node does work which corresponds to the size of the subproblem $n$ passed to that instance of the recursive call and given by $f(n)$. This is important because the time it takes to create subproblems cannot be omitted. The total amount of work is the sum of work of all nodes in the solution tree. This leads us to the formula: <br>\n",
    "$T(n) = aT(n/b) + f(n)$, <br>\n",
    "In words, this means that the running time of the algorithm ($T(n)$) is the running time of the algorithm on a, by a factor ($b$) reduced, subproblem ($T(n/b)$) times the amount of subproblems ($a$) each node has (in the recursion), added to the running time it takes to create subproblems and combine their results ($f(n)$).\n",
    "\n",
    "The master theorem allows to covert this recurrence relation to Big-Theta form. Nevertheless, there are limitations. The master theorem cannot be used if:\n",
    "$T(n)$ is not monotone ($T(n) = sin(n)$) <br>\n",
    "$f(n)$ is not a polynomial ($f(n) = 2n$) <br>\n",
    "$a$ is not a constant ($a=2n$) <br>\n",
    "$a<1$ <br>\n",
    "\n",
    "Examples will be calculated for the corresponding algorithms.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithm Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Searching\n",
    "- the input is a set of $n$ keys $S$, and a query key $q$\n",
    "- the problem is: Where is $q$ in $S$?\n",
    "\n",
    "The following questions help to decide which algorithm should be used:\n",
    "- How much time can you spend programming?\n",
    "- Are certain items accessed more often than other ones?\n",
    "- Might access frequencies change over time?\n",
    "- Is the key close by?\n",
    "- Is my data structure sitting on external memory?\n",
    "- Can I guess where the key should be?  \n",
    "\n",
    "*Chapter 14.2 in The Algorithm Design Manual*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Search\n",
    "- efficient algorithm for finding an item from a sorted list of items\n",
    "- works by repeatedly dividing in half the portion of the list that could contain the item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element is present at index 4\n"
     ]
    }
   ],
   "source": [
    "def binary_search(arr, low, high, x): \n",
    "    \n",
    "    if high >= low:\n",
    "\n",
    "        guess = (high + low) // 2\n",
    "\n",
    "        if arr[guess] == x: \n",
    "            return guess\n",
    "        elif arr[guess] > x:\n",
    "            return binary_search(arr, low, guess - 1, x) \n",
    "        else: \n",
    "            return binary_search(arr, guess + 1, high, x) \n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# Test array \n",
    "arr = [ 2, 3, 4, 10, 40 ] \n",
    "x = 40\n",
    "\n",
    "result = binary_search(arr, 0, len(arr)-1, x) \n",
    "\n",
    "if result != -1: \n",
    "    print(\"Element is present at index\", str(result)) \n",
    "else: \n",
    "    print(\"Element is not present in array\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running time of binary search:\n",
    "- array of 8\n",
    "1. incorrect guess: reduce the size to 4\n",
    "2. incorrect guess: reduce the size to 2\n",
    "3. incorrect guess: reduce the size to 1\n",
    "4. guess: either the last element is the wanted value or it is not in the array\n",
    "\n",
    "To calculate this even faster, we can leverage the $log_2$. The $log_2$ of 8 is 3. This means that we need 3 iterations to have one element left. As seen above, the means that there is one step left: to decide if this is the wanted value or not.\n",
    "This means that we can find the max. iteration count with the formula $log_2(n)+1$. For arrays with a length that is not a power of 2 we can still you this formula. For example, if n is 1000, the $log_2(1000)$ is about 9.97. Adding 1 as in the formula, we get 10.97. In the case of a decimal number, we round down to find the actual number of guesses (in this case 10).\n",
    "\n",
    "This shows that the time complexity of binary search is $O(logn)$. This is true for both the worst and average case. Best case would be $O(1)$ if the first guess is the searched item.\n",
    "\n",
    "Best case in Big-Omega notation would $\\Omega(1)$ as well. Worst-case would be $\\Omega(logn)$, as it would take at least logn many iterations to say that the item is not present.\n",
    "\n",
    "We can see that Big-O and Big-Omega notation have the same values. This means that we can express the running time in Big-Theta notation. Best case is $\\Theta(1)$ and worst and average case are $\\Theta(logn)$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element is present at index 1\n"
     ]
    }
   ],
   "source": [
    "def linear_search(arr, x): \n",
    "  \n",
    "    for i in range(len(arr)): \n",
    "  \n",
    "        if arr[i] == x: \n",
    "            return i \n",
    "  \n",
    "    return -1\n",
    "\n",
    "arr = [ 2, 3, 4, 10, 40 ] \n",
    "x = 3\n",
    "\n",
    "result = linear_search(arr, x) \n",
    "\n",
    "if result != -1: \n",
    "    print(\"Element is present at index\", str(result)) \n",
    "else: \n",
    "    print(\"Element is not present in array\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the linear search algorithm, we assume that this code would need the sum of one iteration times the length of the array plus the time it takes to set up the for loop and returning a value.\n",
    "c1 * n + c2\n",
    "Where c1 is the time for one loop iteration, n the array length and c2 the time of the overhead. As learned in the asymptotic analysis chapter, we can ommit constants.\n",
    "We now can say that the running time grows with n. The notation is $\\Theta(n)$ for average and worst case. Best case would be, of course, $\\Theta(1)$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Sorting\n",
    "- the input is a set of $n$ items\n",
    "- the problem is to arrange the items in increasing/decreasing order\n",
    "\n",
    "The following questions help to decide which algorithm should be used:\n",
    "- How many keys will you be sorting?\n",
    "- Will there be duplicate keys in the data?\n",
    "- What do you know about your data?\n",
    "    - Has the data already been partially sorted?\n",
    "    - Do you know the distribution of the keys?\n",
    "    - Are your keys very long or hard to compare?\n",
    "    - Is the range of possible keys very small?\n",
    "- Do I have to worry about disk accesses?\n",
    "- How much time do you have to write and debug your routine?  \n",
    "\n",
    "*Chapter 14.1 in The Algorithm Design Manual*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selection Sort\n",
    "- sorts an algorithm by repeatedly finding the minimum element from the unsorted part and putting it at the beginning\n",
    "- in place algorithm\n",
    "    - does not save the ordered array in another memory location\n",
    "    - rearranges the input array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 12, 22, 25, 64]\n"
     ]
    }
   ],
   "source": [
    "def selection_sort(arr):\n",
    "    for i in range(len(arr)): \n",
    "\n",
    "        min_idx = i\n",
    "        for j in range(i+1, len(arr)): \n",
    "            if arr[min_idx] > arr[j]: \n",
    "                min_idx = j \n",
    "\n",
    "        arr[i], arr[min_idx] = arr[min_idx], arr[i]\n",
    "    return arr\n",
    "\n",
    "test_arr = [64, 25, 12, 22, 11]\n",
    "\n",
    "print(selection_sort(test_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing this algorithm:\n",
    "- The outer loop runs n times $\\Theta(n)$\n",
    "- the inner loop runs n times then n-1 times then n-2 times ... (arithmetic series)\n",
    "    - $n^2/2+n/2$ calculates the sum of this series\n",
    "    - this means that $\\Theta(n^2)$ applies for the inner loop\n",
    "- as learned, constant factors and lower-order terms do not matter for the asymptotic analysis\n",
    "- the most significant term is $\\Theta(n^2)$\n",
    "    - this means that the running time of selection sort is$\\Theta(n^2)$\n",
    "    - we also know that selection sort always runs in $\\Theta(n^2)$ time in all cases\n",
    "        - this is true because the iteration amount of the loops does not depend on the order of data in the array\n",
    "        - even a sorted array would lead to the same amount of iterations as an unsorted one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insertion Sort\n",
    "- builds the final sorted array one item at a time\n",
    "- efficient for small data sets\n",
    "- in-place algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted array is:\n",
      "[5, 6, 11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "def insertionSort(arr): \n",
    "\n",
    "    for i in range(1, len(arr)): \n",
    "\n",
    "        key = arr[i] \n",
    "        \n",
    "        j = i-1\n",
    "        while j >=0 and key < arr[j] : \n",
    "                arr[j+1] = arr[j] \n",
    "                j -= 1\n",
    "        arr[j+1] = key \n",
    "\n",
    "\n",
    "arr = [12, 11, 13, 5, 6] \n",
    "insertionSort(arr) \n",
    "print (\"Sorted array is:\") \n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing this algorithm:\n",
    "- The outer loop runs n-1 times $\\Theta(n-1)$\n",
    "- the while loop runs k (the length of the subarray) times if the value that is inserted is smaller than all elements in the subarray\n",
    "    - the first time: k = 1, second time: k = 2, ..., the last time: k = n-1\n",
    "    - if c is the constant lines of code which are executed, the following expression calculates the sum:\n",
    "        - $cn^2/2-cn/2$\n",
    "    - this means that $\\Theta(n^2)$ applies for the inner loop\n",
    "- there is still is the possibility that the value that is inserted is bigger than all elements in the subarray\n",
    "    - this means that the loop would only run once and in total, it would run n-1 times\n",
    "    - $\\Theta(n)$\n",
    "- special case \"almost sorted\"\n",
    "    - every element starts at most some constant number (a) of positions from where it's supposed to be when sorted\n",
    "    - at most this many steps would be needed $a * c * (n-1)$\n",
    "    - this means that $\\Theta(n)$ applies\n",
    "    - this means that insertion sort is fast when given an almost-sorted array\n",
    "    \n",
    "summary of the time complexity:\n",
    "- worst case (sorted array in reverse order): $\\Theta(n^2)$\n",
    "- best case (sorted array): $\\Theta(n)$\n",
    "- average case: $\\Theta(n^2)$\n",
    "- \"almost sorted\" case: $\\Theta(n)$\n",
    "- for all cases, we can notate the upper bound with: $O(n^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heapsort\n",
    "If the minimum element of a min-heap data structure is removed, the hole it leaves has to be filled. This can be done by moving the element of the right-most leaf into the first position.\n",
    "This triggers a certain response:\n",
    "- the labeling of the root may no longer satisfy the heap property\n",
    "- the root should be the smallest of the three elements (root and 2 children)\n",
    "- if the root is not the smallest, the smallest of the three has to be the new root\n",
    "- the dissatisfied element will go down the heap until it dominates all its children or becoming a leaf node\n",
    "- this will happen recursively until every node and its children have been rearranged\n",
    "\n",
    "This down-operation is also called heapify. Exchanging the maximum element with the last element and calling heapify\n",
    "repeatedly gives an in-place sorting algorithm, named Heapsort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "def heapify(arr, n, i): \n",
    "    largest = i\n",
    "    l = 2 * i + 1\n",
    "    r = 2 * i + 2\n",
    "  \n",
    "    if l < n and arr[i] < arr[l]: \n",
    "        largest = l \n",
    "  \n",
    "    if r < n and arr[largest] < arr[r]: \n",
    "        largest = r \n",
    "  \n",
    "    if largest != i: \n",
    "        arr[i],arr[largest] = arr[largest],arr[i]\n",
    "  \n",
    "\n",
    "        heapify(arr, n, largest) \n",
    "\n",
    "def heapSort(arr): \n",
    "    n = len(arr) \n",
    "  \n",
    "    for i in range(n//2 - 1, -1, -1): \n",
    "        heapify(arr, n, i) \n",
    "  \n",
    "    for i in range(n-1, 0, -1): \n",
    "        arr[i], arr[0] = arr[0], arr[i]\n",
    "        heapify(arr, i, 0) \n",
    "  \n",
    "test_array = [ 12, 11, 13, 5, 6, 7] \n",
    "heapSort(test_array) \n",
    "print(test_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- time complexity\n",
    "    - $O(nlogn)$ for all cases\n",
    "        - heapify takes $O(logn)$ time and heapSort repeats this $n$ times\n",
    "        \n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Recursive Algorithms\n",
    "\"To solve a problem, solve a subproblem that is a smaller instance of the same problem, and then use the solution to that smaller instance to solve the original problem.\"\n",
    "\n",
    "Recursive algorithms have:\n",
    "- base cases that terminate the loop\n",
    "- recursive cases that call themselves\n",
    "\n",
    "\n",
    "- they can take a lot of memory due to many function calls\n",
    "- improving efficiency of recursive algorithms can be achieved by\n",
    "    - using tail recursion\n",
    "        - the recursive call is the last thing executed by the function\n",
    "        - the compiler optimizes a tail recursive function by not saving the current function's stack frame because the recursive call is the last statement and there is nothing left to do in that function\n",
    "        - Python does [not support](https://stackoverflow.com/questions/13591970/does-python-optimize-tail-recursion) tail recursion (TCO, tail call optimization)\n",
    "    - memoization (a form of caching)\n",
    "        - saving results in a lookup table (e.g. python dict)\n",
    "        - instead of calculating, looking it up\n",
    "    - bottom-up (avoiding recursion)\n",
    "        - using an iterative method to save time and space\n",
    "    - memoization and bottom-up are techniques from dynamic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive Factorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def factorial(n):\n",
    "    if n < 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * factorial(n - 1)\n",
    "    \n",
    "factorial(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tail Recursive Factorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "# example code since Python does not support tail recusion\n",
    "\n",
    "def fact(n, a = 1): \n",
    "  \n",
    "    if (n == 0): \n",
    "        return a \n",
    "  \n",
    "    return fact(n - 1, n * a) \n",
    "\n",
    "print(fact(5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "time complexity:\n",
    "- the amount of recursive calls is directly proportional to the input number\n",
    "- $\\Theta(n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Palindrome Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a palindrome\n"
     ]
    }
   ],
   "source": [
    "def palindrome(word):\n",
    "    if len(word) < 2:\n",
    "        return print(\"a palindrome\")\n",
    "    if word[0] == word[-1]:\n",
    "        return palindrome(word[1:-1])\n",
    "    else:\n",
    "        return print(\"not a palindrome\")\n",
    "    \n",
    "palindrome(\"rotor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "time complexity:\n",
    "- recursive call will be made n/2 times, as the word is stripped of the first and last letter every call\n",
    "- the amount of recursive calls is directly proportional to the input number\n",
    "- $\\Theta(n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive Fibonacci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "def fib(n):\n",
    "    if n==1: \n",
    "        return 0\n",
    "    elif n==2: \n",
    "        return 1\n",
    "    else: \n",
    "        return fib(n-1)+fib(n-2) \n",
    "\n",
    "print(fib(8)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recursive fibonacci has an interesting time complexity. <br>\n",
    "The calls can be expressed as F(n) = F(n-1) + F(n-2). Expressing this as a tree we get something like this:\n",
    "```\n",
    "0                        n\n",
    "1            (n-1)                 (n-2)\n",
    "2        (n-2)    (n-3)      (n-3)     (n-4)\n",
    "3   (n-3)(n-4) (n-4)(n-5) (n-4)(n-5) (n-5)(n-6)\n",
    "```\n",
    "We are now looking for a function that satisfies these rules. We can use\n",
    "$a^n = a^{n-1} + a^{n-2}$. This is based of the assumption that we have a to the power of n nodes in the tree that calculate the running time of the algorithm. In the tree above, we could assume to have $2^3$ nodes if the n input is 3. The problem is that the height might be n but the average number of branches is less than 2 due to the base cases. To calculate the exact value, we do: <br>\n",
    "$a^n = a^{n-1} + a^{n-2}$ divide through by $a^{n-2}$ <br>\n",
    "$a^2 = a + 1$\n",
    "Solving this with the quadratic formula we get a=1.61803 and a=-0.618303 (rounded). A negative solution would not make sense here since the running time cannot be negative. Thus, the answer is a=1.61803. This means that we can express the time complexity of this algorithm as $\\Theta(1.61803^n)$.\n",
    "\n",
    "Another interesting fact is that this value is called the [\"golden ratio\"](https://en.wikipedia.org/wiki/Golden_ratio)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bottom-up Fibonacci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "def fib_bottom_up(n): \n",
    "    a = 0\n",
    "    b = 1\n",
    "    if n == 1: \n",
    "        return 0\n",
    "    elif n == 2: \n",
    "        return 1\n",
    "    else: \n",
    "        for i in range(2,n): \n",
    "            c = a + b \n",
    "            a = b \n",
    "            b = c \n",
    "        return c\n",
    "\n",
    "print(fib_bottom_up(8)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that the loop runs n-2 times, thus the time complexity of the bottom-up approach is $\\Theta(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tower of Hanoi Algorithm (Multiple Recursion)\n",
    "- recursion that contain a single self-reference is known as single recursion\n",
    "- recursion with multiple self-references is known as multiple recusion\n",
    "- another example would be the recursive fibonacci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Move disk 1 from rod A to rod C\n",
      "Move disk 2 from rod A to rod B\n",
      "Move disk 1 from rod C to rod B\n",
      "Move disk 3 from rod A to rod C\n",
      "Move disk 1 from rod B to rod A\n",
      "Move disk 2 from rod B to rod C\n",
      "Move disk 1 from rod A to rod C\n",
      "Move disk 4 from rod A to rod B\n",
      "Move disk 1 from rod C to rod B\n",
      "Move disk 2 from rod C to rod A\n",
      "Move disk 1 from rod B to rod A\n",
      "Move disk 3 from rod C to rod B\n",
      "Move disk 1 from rod A to rod C\n",
      "Move disk 2 from rod A to rod B\n",
      "Move disk 1 from rod C to rod B\n",
      "Move disk 5 from rod A to rod C\n",
      "Move disk 1 from rod B to rod A\n",
      "Move disk 2 from rod B to rod C\n",
      "Move disk 1 from rod A to rod C\n",
      "Move disk 3 from rod B to rod A\n",
      "Move disk 1 from rod C to rod B\n",
      "Move disk 2 from rod C to rod A\n",
      "Move disk 1 from rod B to rod A\n",
      "Move disk 4 from rod B to rod C\n",
      "Move disk 1 from rod A to rod C\n",
      "Move disk 2 from rod A to rod B\n",
      "Move disk 1 from rod C to rod B\n",
      "Move disk 3 from rod A to rod C\n",
      "Move disk 1 from rod B to rod A\n",
      "Move disk 2 from rod B to rod C\n",
      "Move disk 1 from rod A to rod C\n"
     ]
    }
   ],
   "source": [
    "def TowerOfHanoi(n , from_rod, to_rod, aux_rod):\n",
    "    if n == 1:\n",
    "        return print(\"Move disk 1 from rod\",from_rod,\"to rod\",to_rod)\n",
    "    TowerOfHanoi(n-1, from_rod, aux_rod, to_rod)\n",
    "    print(\"Move disk\",n,\"from rod\",from_rod,\"to rod\",to_rod)\n",
    "    TowerOfHanoi(n-1, aux_rod, to_rod, from_rod)\n",
    "\n",
    "TowerOfHanoi(5, 'A', 'C', 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the time complexity of this algorithm. <br>\n",
    "The recursive equation can be expressed as: <br>\n",
    "I $T(n)=2T(n-1)+1$ <br>\n",
    "II $T(n-1)=2T(n-2)+1$ <br>\n",
    "III $T(n-2)=2T(n-3)+1$ <br>\n",
    "\n",
    "Now we can substitute $T(n-2)$ in equation II with the term of equation III: <br>\n",
    "IV $T(n-1)=2(2T(n-3)+1)+1$ <br>\n",
    "\n",
    "Now we can substitute $T(n-1)$ in equation I with the term of equation IV: <br>\n",
    "$T(n)=2(2(2T(n-3)+1)+1)+1$ <br>\n",
    "$T(n)=2^3T(n-3)+2^2+2^1+1$ <br>\n",
    "Generalizing this term we get: <br>\n",
    "$T(n)=2^kT(n-k)+2^{k-1}+2^{k-2}+...+1$ <br>\n",
    "With the base case $T(0)=1$, we get that $n-k=0$. This allows us to put n for k: <br>\n",
    "$T(n)=2^nT(0)+2^{n-1}+2^{n-2}+...+1$ <br>\n",
    "The sum of this geometric progression can be expressed as: <br>\n",
    "$2^{n+1}-1$ <br>\n",
    "Ignoring the constants we get the final time complexity of $\\Theta(2^n)$.\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Divide and Conquer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Sort\n",
    "- sorting an array\n",
    "\n",
    "p and r define the subarray\n",
    "1. Divide by finding the number q of the position midway between p and r: add p and r, divide by 2, and round down.\n",
    "2. Conquer by recursively sorting the subarrays in each of the two subproblems created by the divide step. That is, recursively sort the subarray array[p..q] and recursively sort the subarray array[q+1..r].\n",
    "3. Combine by merging the two sorted subarrays back into the single sorted subarray array[p..r].\n",
    "\n",
    "<img src=\"https://cdn.kastatic.org/ka-perseus-images/ace963383bea8d154f6abd1322a06a73b56b4628.png\">\n",
    "<p style=\"text-align: center;\"><a href=\"https://cdn.kastatic.org/ka-perseus-images/ace963383bea8d154f6abd1322a06a73b56b4628.png\">Source</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "def mergeSort(arr): \n",
    "    if len(arr) > 1: \n",
    "        mid = len(arr)//2\n",
    "        L = arr[:mid]\n",
    "        R = arr[mid:]\n",
    "\n",
    "        mergeSort(L)\n",
    "        mergeSort(R)\n",
    "\n",
    "        i = j = k = 0\n",
    "        \n",
    "        while i < len(L) and j < len(R): \n",
    "            if L[i] < R[j]: \n",
    "                arr[k] = L[i] \n",
    "                i+= 1\n",
    "            else: \n",
    "                arr[k] = R[j] \n",
    "                j+= 1\n",
    "            k+= 1\n",
    "        while i < len(L): \n",
    "            arr[k] = L[i] \n",
    "            i+= 1\n",
    "            k+= 1\n",
    "        \n",
    "        while j < len(R): \n",
    "            arr[k] = R[j] \n",
    "            j+= 1\n",
    "            k+= 1\n",
    "\n",
    "arr = [12, 11, 13, 5, 6, 7] \n",
    "mergeSort(arr) \n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time complexity:\n",
    "1. The divide step takes constant time, regardless of the subarray size. After all, the divide step just computes the midpoint q of the indices p and r. This means $\\Theta(1)$.\n",
    "2. The conquer step, where we recursively sort two subarrays of approximately $n/2$ elements each, takes some amount of time, but we'll account for that time when we consider the subproblems.\n",
    "3. The combine step merges a total of n elements, taking $\\Theta(n)$ time.\n",
    "\n",
    "<img src=\"https://cdn.kastatic.org/ka-perseus-images/5fcbebf66560d8fc490de2a0d8a0e5b1d65c5c54.png\">\n",
    "<p style=\"text-align: center;\"><a href=\"https://cdn.kastatic.org/ka-perseus-images/5fcbebf66560d8fc490de2a0d8a0e5b1d65c5c54.png\">Source</a></p>\n",
    "\n",
    "The running time is the sum of the merging times for all the levels.\n",
    "If there are $l$ levels, the total merging time is $l*cn$. $l$ can be written as $log_2n+1$. This means that $\\Theta(nlog_2n)$ is the running time for merge sort in $\\Theta$ notation. This applies for best, worst and average case.\n",
    "\n",
    "We can also use the master theorem to calculate the time complexity:\n",
    "$T(n) = aT(n/b) + f(n)$, <br>\n",
    "where, <br>\n",
    "n = size of input <br>\n",
    "a = number of subproblems in the recursion <br>\n",
    "n/b = size of each subproblem. All subproblems are assumed \n",
    "     to have the same size. <br>\n",
    "f(n) = cost of the work done outside the recursive call, \n",
    "      which includes the cost of dividing the problem and\n",
    "      cost of merging the solutions\n",
    "      \n",
    "As stated in 2. we recursively sort two subproblems of the size $n/2$. This means that we have a = 2 and b = 2. n stays n and f(n) is the cost of dividing and cost of merging. As stated in 1. and 3. this is $\\Theta(1)$ and $\\Theta(n)$. Putting it all together the formula for merge sort becomes: <br>\n",
    "$T(n) = 2T(n/2) + \\Theta(n)$, <br>\n",
    "In this case, we use the second case of the master theorem which states: <br>\n",
    "2. If f(n) = $Θ(n^{log_b a})$, then T(n) = $Θ(n^{log_b a} *log n)$  \n",
    "\n",
    "Let us see if this is true: <br>\n",
    "f(n) = $Θ(n^{log_b a})$ becomes: <br>\n",
    "n = $Θ(n^{log_2 2})$  \n",
    "n = $Θ(n^{1})$  \n",
    "n = $Θ(n)$  \n",
    "This is true and thus, the time complexity is: <br>\n",
    "T(n) = $Θ(n^{log_2 2} *log n)$ <br>\n",
    "T(n) = $Θ(n^{1} *log n)$ <br>\n",
    "T(n) = $Θ(n*log n)$\n",
    "\n",
    "Comparing this result with the previous one, we see that both are the same.\n",
    "\n",
    "\n",
    "Also to note:\n",
    "- during merging, two copies of the entire array are made (L and R)\n",
    "- because it copies more than a constant number of elements at some time, merge sort does not work in place\n",
    "- if space is limited, in-place algorithms are preferred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quicksort\n",
    "- sorting an array\n",
    "- in place algorithm  \n",
    "\n",
    "\n",
    "1. Divide by choosing any element in the subarray array [p..r] (the pivot). Rearrange the elements in array [p..r] so that all elements in array [p..r] that are less than or equal to the pivot are to its left and all elements that are greater than the pivot are to its right (partitioning).\n",
    "2. Conquer by recursively sorting the subarrays array [p..q-1] and array [q+1..r].\n",
    "3. Combine by doing nothing. The elements in array[p..r] can't help but be sorted!\n",
    "\n",
    "<img src=\"https://cdn.kastatic.org/ka-perseus-images/9876d4dc59e01a4742860ae1831c20f654ed7959.png\">\n",
    "<p style=\"text-align: center;\"><a href=\"https://cdn.kastatic.org/ka-perseus-images/9876d4dc59e01a4742860ae1831c20f654ed7959.png\">Source</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted array is:\n",
      "[7, 8, 9, 10, 21, 50]\n"
     ]
    }
   ],
   "source": [
    "def partition(arr,low,high): \n",
    "    i = ( low-1 )\n",
    "    pivot = arr[high]\n",
    "  \n",
    "    for j in range(low , high): \n",
    "        if   arr[j] <= pivot: \n",
    "          \n",
    "            i = i+1 \n",
    "            arr[i],arr[j] = arr[j],arr[i] \n",
    "  \n",
    "    arr[i+1],arr[high] = arr[high],arr[i+1] \n",
    "    return ( i+1 ) \n",
    "\n",
    "def quickSort(arr,low,high): \n",
    "    if low < high: \n",
    "        q = partition(arr,low,high) \n",
    "        quickSort(arr, low, q-1) \n",
    "        quickSort(arr, q+1, high) \n",
    "        \n",
    "arr = [10, 7, 8, 9, 21, 50] \n",
    "n = len(arr) \n",
    "quickSort(arr,0,n-1) \n",
    "print (\"Sorted array is:\") \n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "worst case running time:\n",
    "- partitions are the most unbalanced\n",
    "$\\Theta(n^2)$\n",
    "<img src=\"https://cdn.kastatic.org/ka-perseus-images/7da2ac32779bef669a6f05decb62f219a9132158.png\">\n",
    "<p style=\"text-align: center;\"><a href=\"https://cdn.kastatic.org/ka-perseus-images/7da2ac32779bef669a6f05decb62f219a9132158.png\">Source</a></p>\n",
    "\n",
    "\n",
    "best case running time:\n",
    "- partitions are evenly balanced (same or difference of 1)\n",
    "$\\Theta(nlogn)$\n",
    "<img src=\"https://cdn.kastatic.org/ka-perseus-images/21cd0d70813845d67fbb11496458214f90ad7cb8.png\">\n",
    "<p style=\"text-align: center;\"><a href=\"https://cdn.kastatic.org/ka-perseus-images/21cd0d70813845d67fbb11496458214f90ad7cb8.png\">Source</a></p>\n",
    "\n",
    "average case running time:\n",
    "- math explained [here](https://www.khanacademy.org/computing/computer-science/algorithms/quick-sort/a/analysis-of-quicksort)\n",
    "$O(nlogn)$\n",
    "<img src=\"https://cdn.kastatic.org/ka-perseus-images/130b2d2a1fe897253def054f4c3aa7bd94cb6cf2.png\">\n",
    "<p style=\"text-align: center;\"><a href=\"https://cdn.kastatic.org/ka-perseus-images/130b2d2a1fe897253def054f4c3aa7bd94cb6cf2.png\">Source</a></p>\n",
    "\n",
    "We can also use the master theorem to calculate the time complexity:\n",
    "$T(n) = aT(n/b) + f(n)$, <br>\n",
    "where, <br>\n",
    "n = size of input <br>\n",
    "a = number of subproblems in the recursion <br>\n",
    "n/b = size of each subproblem. All subproblems are assumed \n",
    "     to have the same size. <br>\n",
    "f(n) = cost of the work done outside the recursive call, \n",
    "      which includes the cost of dividing the problem and\n",
    "      cost of merging the solutions\n",
    "      \n",
    "We can see that the number of subproblems in the recursion is of the size 2. Furthermore, we get subproblems with a size of $n/2$ since the pivot rearranges the array with respect to bigger and smaller values. The cost of the work that is done outside of the recursive calls, which is partitioning the array, has a time complexity of $\\Theta(n)$. This leaves us with the variables a=2, b=2 and f(n)=n. <br>\n",
    "$T(n) = 2T(n/2) + \\Theta(n)$, <br> \n",
    "These are the same values as for the merge sort algorithm. Thus, we can say that we have a time complexity of: <br>\n",
    "T(n) = $\\Theta(n*log n)$ <br>\n",
    "\n",
    "Nevertheless, there is a difference. The recurrence above applies for the best case. For the worst case, we have a different recurrence: <br>\n",
    "As we can see in the worst case tree graphic above, in this case we choose a pivot that returns the most unbalanced partitions. This means that we only have one recursive call and the subproblems are of the size n-1. The partitioning function still has $\\Theta(n)$.<br>\n",
    "$T(n) = T(n-1) + \\Theta(n)$, <br> \n",
    "\n",
    "The master theorem is designed for the case where\n",
    "1. the number of subproblems is a constant, and\n",
    "2. the sizes of the subproblems decay geometrically\n",
    "\n",
    "The recurrence for the worst case has the problem size decay linearly and thus, the second requirement does not hold. This means, that we cannot solve this recurrence by using the master theorem.\n",
    "We can still unroll the recurrence ourselves to solve it: <br>\n",
    "$\\Theta(n+(n-1)+(n-2)+...+2+1)$ <br>\n",
    "$\\Theta(n(n+1)/2)$ <br>\n",
    "$\\Theta(n^2)$ <br>\n",
    "\n",
    "Comparing this result with the previous one, we see that both are the same.\n",
    "\n",
    "\n",
    "Last, for the average case we get the recurrence: <br>\n",
    "$T(n) = 1/n * sum[T(i) + T(n-i-1), i=0...n-1] + \\Theta(n)$, <br>\n",
    "This again cannot be solved by the master theorem but by unrolling again. The term can be summarized by: <br>\n",
    "$O(n*logn)$.<br>\n",
    "The exact math can be found [here](http://www.hananayad.com/teaching/syde423/quickSortAvgCase.pdf)\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Breadth-First Search\n",
    "#### General\n",
    "- example application: path finding\n",
    "- visits each connected node of the current node\n",
    "    - 1st degree neighbors first\n",
    "    - 2nd degree neighbors second\n",
    "    - ...\n",
    "    - thus, finds the nearest goal neighbor (shortest path)\n",
    "- terminates when it finds a/the goal node\n",
    "- keeps track of nodes that have already been visited but have not yet been visited from\n",
    "    - queue (first in, first out)\n",
    "\n",
    "#### Analysis of BFS\n",
    "- graph with Node set N and Edge set E\n",
    "\n",
    "- running time is $O(V+E)$:\n",
    "    - since every node and edge will be explored in the worst case\n",
    "    - $O(E)$ may very between $O(1)$ and $O(V^2)$ (adjacency matrix), depending on what the input graph is\n",
    "\n",
    "\n",
    "meaning of $O(V+E)$\n",
    "- for $\\lvert E \\rvert \\ge \\lvert V \\rvert$\n",
    "    - $\\lvert E \\rvert + \\lvert V \\rvert \\le \\lvert E \\rvert + \\lvert E \\rvert = 2*\\lvert E \\rvert$\n",
    "    - ignoring constant factors in asymptotic notation, we get $O(E)$\n",
    "- for $\\lvert E \\rvert \\lt \\lvert V \\rvert$\n",
    "    - $\\lvert V \\rvert + \\lvert E \\rvert \\le \\lvert V \\rvert + \\lvert V \\rvert = 2*\\lvert V \\rvert$\n",
    "    - ignoring constant factors in asymptotic notation, we get $O(V)$\n",
    "- this means that $(O(V+E)$ really means $O(max(V,E))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing: {'toaster_has_power': False, 'toaster_is_on': False, 'bread_location': 'plate', 'bread_state': 'untoasted', 'time': 0}\n",
      "BFS needed 2165 loop iterations.\n",
      "found sequence: ['plug_in_toaster', 'put_in_bread', 'switch_on_toaster', 'wait', 'take_out_bread']\n",
      "fulfills goal? True\n",
      "in world time 14\n"
     ]
    }
   ],
   "source": [
    "actions=(\n",
    "    \"plug_in_toaster\",\n",
    "    \"unplug_toaster\",\n",
    "    \"put_in_bread\",\n",
    "    \"take_out_bread\",\n",
    "    \"switch_on_toaster\",\n",
    "    \"wait\")\n",
    "\n",
    "state={\n",
    "    \"toaster_has_power\":False,\n",
    "    \"toaster_is_on\":False,\n",
    "    \"bread_location\":\"plate\",\n",
    "    \"bread_state\":\"untoasted\",\n",
    "    \"time\":0\n",
    "    }\n",
    "\n",
    "def goal(state):\n",
    "    return state[\"bread_location\"] == \"plate\" and state[\"bread_state\"] == \"toasted\"\n",
    "\n",
    "def state_transition(state, action):\n",
    "    newState = state.copy()\n",
    "    if action ==\"plug_in_toaster\":\n",
    "        # toaster now has power\n",
    "        newState[\"toaster_has_power\"] = True\n",
    "        newState[\"time\"] += 1\n",
    "    elif action ==\"unplug_toaster\":\n",
    "        # unpower toaster and stop toasting process\n",
    "        newState[\"toaster_has_power\"] = False\n",
    "        newState[\"toaster_is_on\"] = False\n",
    "        newState[\"time\"] += 1\n",
    "    elif action == \"put_in_bread\":\n",
    "        # move bread into toaster. Only possible if toaster is not on (casue it locks)\n",
    "        if not newState[\"toaster_is_on\"]:\n",
    "            newState[\"bread_location\"] = \"toaster\"\n",
    "        newState[\"time\"] += 1\n",
    "    elif action == \"take_out_bread\":\n",
    "        # move bread from toaster to plate. Only possible if toaster is not on (casue it locks)\n",
    "        if not newState[\"toaster_is_on\"]:\n",
    "            newState[\"bread_location\"]=\"plate\"\n",
    "        newState[\"time\"] += 1\n",
    "    elif action == \"switch_on_toaster\": \n",
    "        # switch on the toaster\n",
    "        if newState[\"toaster_has_power\"]:\n",
    "            newState[\"toaster_is_on\"]=True\n",
    "        newState[\"time\"] += 1\n",
    "    elif action == \"wait\":\n",
    "        # wait for ten steps\n",
    "        newState[\"time\"] += 10\n",
    "        # if toaster is on, it is switched off, if bread was in toaster, it is toasted now.\n",
    "        if newState[\"toaster_is_on\"]:\n",
    "            if newState[\"bread_location\"] == \"toaster\":\n",
    "                newState[\"bread_state\"] = \"toasted\"\n",
    "            newState[\"toaster_is_on\"] = False\n",
    "    return newState\n",
    "\n",
    "\n",
    "# BFS algorithm\n",
    "def plan(start_state):\n",
    "\n",
    "    to_visit = [(start_state, [])]\n",
    "    loop_runs = 0\n",
    "    counter = 0\n",
    "\n",
    "    while(to_visit):\n",
    "        loop_runs += 1\n",
    "        counter += 1\n",
    "\n",
    "        (state, path) = to_visit.pop(0)\n",
    "\n",
    "        if goal(state):\n",
    "            print(\"BFS needed \" + str(loop_runs) + \" loop iterations.\")\n",
    "            return path\n",
    "\n",
    "        for action in actions:\n",
    "            to_visit.append((state_transition(state, action), path + [action]))\n",
    "\n",
    "            \n",
    "def test(start_state):\n",
    "    print(\"testing:\",start_state)\n",
    "\n",
    "    # call plan function\n",
    "    #sequence = plan(start_state)\n",
    "    sequence = plan(start_state)\n",
    "    print(\"found sequence:\",sequence)\n",
    "\n",
    "    # apply plan to start state\n",
    "    state = start_state\n",
    "    for action in sequence:\n",
    "        state = state_transition(state,action)\n",
    "    # check whether result fulfills the goal\n",
    "    print(\"fulfills goal?\", goal(state))\n",
    "    print(\"in world time\",state[\"time\"])\n",
    "\n",
    "# execute the test for a few test cases\n",
    "test(state)\n",
    "# test({'toaster_has_power': True, 'toaster_is_on': False, 'bread_location': 'toaster', 'bread_state': 'untoasted', 'time': 0})\n",
    "# test({'toaster_has_power': True, 'toaster_is_on': True, 'bread_location': 'plate', 'bread_state': 'untoasted', 'time': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a planning AI used to find the right action sequence to toast bread. It was a homework task for the AI module.\n",
    "In this case, the graph nodes are different states. The BFS algorithm finds the nearest goal statee. Due to the time parameter, this might be the nearest state but not in regards to the time of the \"toaster world\" (graph is weighted but for this example the weights can be ignored)\n",
    "\n",
    "State neighbors (new nodes) are found by applying the state transition function on the current state (current node). The resulting state, is the neighbor of the previous state and thus, can be added to the queue.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Depth-First Search\n",
    "#### General\n",
    "- example: path finding\n",
    "- explores as far as possible along each subtree before backtracking\n",
    "- terminates when it finds a goal node\n",
    "- keeping track of nodes that have already been visited but have not yet been visited from\n",
    "    - stack (last in, first out)\n",
    "\n",
    "#### Analysis of DFS\n",
    "- running time is the same as BFS\n",
    "    - a difference that DFS may never terminate (see example below)\n",
    "    - BFS will always find a solution (may take a lot of time), DFS might not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DFS algorithm\n",
    "def plan(start_state):\n",
    "\n",
    "    to_visit = [(start_state, [])]\n",
    "    loop_runs = 0\n",
    "    counter = 0\n",
    "\n",
    "    while(to_visit):\n",
    "        loop_runs += 1\n",
    "        counter += 1\n",
    "\n",
    "        (state, path) = to_visit.pop(0)\n",
    "\n",
    "        if goal(state):\n",
    "            print(\"BFS needed \" + str(loop_runs) + \" loop iterations.\")\n",
    "            return path\n",
    "\n",
    "        for action in actions:\n",
    "            to_visit.insert(0, (state_transition(state, action), path + [action]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see a code snippet of DFS that could be used for the AI task of the BFS example. The problem is that the tree structure of the state space is continuously growing as we apply the state transition function on our current state (node). Thus, we need to hope that either the state transition function cannot be applied at some point or that the goal state is found. Otherwise, this search will never terminate.  \n",
    "\n",
    "The only difference between DFS and BFS is the adding of new nodes to the to_visit data structure. \n",
    "```\n",
    "to_visit.append((state_transition(state, action), path + [action]))\n",
    "# first in, first out logic (queue)\n",
    "```\n",
    "```\n",
    "to_visit.insert(0, (state_transition(state, action), path + [action]))\n",
    "# last in, first out logic (stack)\n",
    "```\n",
    "\n",
    "(There is better solution for Python queues which are of the ```collections.deque``` class but I found the solution above shows the difference more clearly.)\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Greedy Algorithms\n",
    "- at each step one picks the locally optimal move\n",
    "- faster but not optimal (local optimum which can be a global optimum)\n",
    "- optimization\n",
    "\n",
    "approximation algorithms are judged by:\n",
    "- how fast they are\n",
    "- how close they are to the optimal solution\n",
    "\n",
    "#### Example Algorithm\n",
    "You start a radio show and want to reach as many people in all 50 states as possible. You have to decide what stations to play on to reach many people. It costs money to be on each startion. How to reach as many people and spend the least amount of money?\n",
    "\n",
    "1. List every possible subset of stations.\n",
    "    - this is calculated by $2^n$, where n is 50\n",
    "2. Pick the set with the smallest number of stations that covers all 50 states.\n",
    "\n",
    "This means that it takes $O(2^n)$ time. For a large n this can take a long time to be solved. This is where greedy (approximation) algorithms are useful.\n",
    "\n",
    "1. Pick the station that covers the most states that haven't been covered yet. Overlap does not matter.\n",
    "2. Repeat until all states are covered.\n",
    "\n",
    "In this case the greedy algorithms runs in $O(n^2)$ time.\n",
    "\n",
    "This type of problem is called NP-complete problem:\n",
    "- nondeterministic polynomial complete problem\n",
    "- algorithms will not solve these problems quickly\n",
    "- see radio problem above for a large n\n",
    "- if you deal with a NP-complete problem\n",
    "    - solve it using approximation\n",
    "- it is not easy to define NP-completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ktwo', 'kone', 'kthree', 'kfive'}\n"
     ]
    }
   ],
   "source": [
    "# example greedy algorithm\n",
    "\n",
    "states_needed = set([\"mt\", \"wa\", \"or\", \"id\", \"nv\", \"ut\", \"ca\", \"az\"])\n",
    "\n",
    "stations = {}\n",
    "stations[\"kone\"] = set([\"id\", \"nv\", \"ut\"])\n",
    "stations[\"ktwo\"] = set([\"wa\", \"id\", \"mt\"])\n",
    "stations[\"kthree\"] = set([\"or\", \"nv\", \"ca\"])\n",
    "stations[\"kfour\"] = set([\"nv\", \"ut\"])\n",
    "stations[\"kfive\"] = set([\"ca\", \"az\"])\n",
    "\n",
    "final_stations = set()\n",
    "\n",
    "while states_needed:\n",
    "    best_station = None\n",
    "    states_covered = set()\n",
    "    for station, states_for_station in stations.items():\n",
    "        covered = states_needed & states_for_station\n",
    "        if len(covered) > len(states_covered):\n",
    "            best_station = station\n",
    "            states_covered = covered\n",
    "\n",
    "    states_needed -= states_covered\n",
    "    final_stations.add(best_station)\n",
    "\n",
    "print(final_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dijkstra's Algorithm\n",
    "- works with weighted graphs\n",
    "    - weights are positive\n",
    "    - for negative weights use Bellman-Ford algorithm\n",
    "- finds the path with the smallest total weight\n",
    "- only works with directed acyclic graphs\n",
    "\n",
    "How it works:\n",
    "1. Finds the \"cheapest\" node.\n",
    "2. Update costs of neighboring nodes\n",
    "3. Repeat for every node\n",
    "4. Calculate final path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost from the start to each node:\n",
      "{'a': 5, 'b': 2, 'fin': 6}\n"
     ]
    }
   ],
   "source": [
    "# the graph\n",
    "graph = {}\n",
    "graph[\"start\"] = {}\n",
    "graph[\"start\"][\"a\"] = 6\n",
    "graph[\"start\"][\"b\"] = 2\n",
    "\n",
    "graph[\"a\"] = {}\n",
    "graph[\"a\"][\"fin\"] = 1\n",
    "\n",
    "graph[\"b\"] = {}\n",
    "graph[\"b\"][\"a\"] = 3\n",
    "graph[\"b\"][\"fin\"] = 5\n",
    "\n",
    "graph[\"fin\"] = {}\n",
    "\n",
    "\n",
    "infinity = float(\"inf\")\n",
    "costs = {}\n",
    "costs[\"a\"] = 6\n",
    "costs[\"b\"] = 2\n",
    "costs[\"fin\"] = infinity\n",
    "\n",
    "\n",
    "parents = {}\n",
    "parents[\"a\"] = \"start\"\n",
    "parents[\"b\"] = \"start\"\n",
    "parents[\"fin\"] = None\n",
    "\n",
    "processed = []\n",
    "\n",
    "def find_lowest_cost_node(costs):\n",
    "    lowest_cost = float(\"inf\")\n",
    "    lowest_cost_node = None\n",
    "    \n",
    "    for node in costs:\n",
    "        cost = costs[node]\n",
    "        \n",
    "        if cost < lowest_cost and node not in processed:\n",
    "            \n",
    "            lowest_cost = cost\n",
    "            lowest_cost_node = node\n",
    "    return lowest_cost_node\n",
    "\n",
    "\n",
    "node = find_lowest_cost_node(costs)\n",
    "\n",
    "while node is not None:\n",
    "    cost = costs[node]\n",
    "    \n",
    "    neighbors = graph[node]\n",
    "    for n in neighbors.keys():\n",
    "        new_cost = cost + neighbors[n]\n",
    "        \n",
    "        if costs[n] > new_cost:\n",
    "            \n",
    "            costs[n] = new_cost\n",
    "            \n",
    "            parents[n] = node\n",
    "            \n",
    "    processed.append(node)\n",
    "    \n",
    "    node = find_lowest_cost_node(costs)\n",
    "\n",
    "print(\"Cost from the start to each node:\")\n",
    "print(costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time complexity:\n",
    "\n",
    "The general time complexity of the Dijkstra's algorithm for any data structure of the vertex set $Q$ is: <br>\n",
    "$\\Theta(\\lvert E \\rvert *T_{dk}+\\lvert V \\rvert*T_{em})$ <br>\n",
    "$T_{dk}$ and $T_{em}$ represent the complexities of the decrease-key and extract-minimum operations in $Q$.\n",
    "\n",
    "As this statement shows, the time complexity is quiet sensitive to data structures that are used to implement the algorithm. Furthermore, the difference between a adjacenty list or matrix have an impact on the running time of the Dijkstra's algorithm. I think that these sources ([1](https://stackoverflow.com/questions/26547816/understanding-time-complexity-calculation-for-dijkstra-algorithm), [2](https://www.geeksforgeeks.org/dijkstras-algorithm-for-adjacency-list-representation-greedy-algo-8/#:~:text=We%20have%20discussed%20Dijkstra's%20algorithm,O(V%5E2)), [3](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm#Running_time)) explain the calculations very well.  \n",
    "\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 ASTAR (A*) Algorithm\n",
    "- can be seen as an extension of the Dijkstra's algorithm\n",
    "- is a backtracking algorithm\n",
    "- uses heuristics to achieve better perforance  \n",
    "\n",
    "\n",
    "- the algorithm uses three functions for each node n\n",
    "    - $g(n)$: the cost from the start node to n\n",
    "    - $h(n)$ (heuristic function): estimates the cost of the cheapest path from n to the goal\n",
    "    - $f(n) = g(n) + h(n)$\n",
    "- A* selects the path that minimizes the $f(n)$  \n",
    "\n",
    "\n",
    "- the A* algorithm is\n",
    "    - complete (if there is a solution, it will be found)\n",
    "    - optimal (the optimal solution will be found, if there are more than one, one of them, dependent on the implementation, will be found)\n",
    "    - optimal efficient (there is no algorithm that will find the solution faster with the same heuristic)  \n",
    "    \n",
    "    \n",
    "- the time complexity only has low importance as the strength is the heuristic search and most nodes will not be visited\n",
    "- as an example, the time complexity can be meaningful for path finding tasks in a maze as A* struggels with the heuristic search\n",
    "    - there have to be simplified assumptions\n",
    "        - monotone heuristic: estimate is always less than or equal to the estimated distance from any neighbouring vertex to the goal, plus the cost of reaching that neighbour\n",
    "        - implementation of the open and closed list have to be efficient data structures\n",
    "            - open list as a binary heap, closed list as an array\n",
    "- with the these assumptions, the worst-case running time is $O(V^2)$  \n",
    "\n",
    "\n",
    "- more generally the time complexity is noted as $O(b^d)$\n",
    "    - b is the branching factor of the tree (average number of successors per state)\n",
    "    - d is the depth of the goal node\n",
    "- one drawback is the space complexity \n",
    "    - all generated nodes are saved in memory (open and closed list)\n",
    "    - this means that the worst-case has a space complexity of $O(b^d)$\n",
    "    \n",
    "The code below was a homework for the AI guild. This is my solution and there was no general solution. This means, that this might not be the optimal heuristic.\n",
    "Usually for path finding, the values for every node are assigned as stated above. This was not possible since the tree structure we get from the state space does not have a \"distance\" that can be calculated to find the heuristic value. I tried to solve this problem by doing the following:\n",
    "1. The g-value of the new node is the time-factor of the current state\n",
    "2. The heuristic is the estimated \"distance\" from the current state to the goal state.\n",
    "To achieve that, I assigned a counter to the node if the state come closer to the goal state. I could not just look at the state and compare it because some states changed from False to True to False in order to change other states. This meant, that I needed to know if the state is currently in the first or second False state. To do that, I used logic to reason and assign a value:  \n",
    "\n",
    "\n",
    "```python\n",
    "#testing and keeping track if actions that need multiple times (false, true, false) \n",
    "if action == \"put_in_bread\" and current_node.state[\"bread_location\"] == \"plate\" and node_state[\"bread_location\"] == \"toaster\":\n",
    "    cbreadloc += 1\n",
    "\n",
    "if action == \"take_out_bread\" and current_node.state[\"bread_location\"] == \"toaster\" and current_node.state[\"toaster_is_on\"] == False and current_node.state[\"bread_state\"] == \"toasted\":\n",
    "    cbreadloc += 2\n",
    "\n",
    "if action == \"switch_on_toaster\" and node_state[\"toaster_is_on\"] == True and current_node.state[\"toaster_is_on\"] == False:\n",
    "    cswitch += 1\n",
    "\n",
    "if action == \"wait\" and current_node.state[\"toaster_has_power\"] == True and current_node.state[\"toaster_is_on\"] == True:\n",
    "    cswitch += 2\n",
    "```\n",
    "    After that, I was able to assign the right heuristic value to the node.\n",
    "3. The last step was to calculate the total node cost. I noticed that if I do $f(n) = g(n) + h(n)$, the node with the right action will be punished. This is because the wait action, which is necessary to complete the algorithm, takes 10 time units. All other actions only use 1 time unit. This means that the state with wait action has a higher g-value than most of the other states' g-values. Thus, the node is not selected until it's total cost is lower than all the nodes in the the open list. To solve this, I calculated the total node cost as $f(n) = g(n) - h(n)$. With the reasoning above, the counter indicated what heuristic value has to be assigned to balance out the wait action time punishment.\n",
    "```python\n",
    "if key == \"toaster_is_on\":\n",
    "    if current_node.cswitch == 1:\n",
    "        h_count += 1\n",
    "    elif current_node.cswitch > 1:\n",
    "        h_count += 9\n",
    "elif key == \"bread_location\":\n",
    "    if current_node.cbreadloc == 1:9\n",
    "        h_count += 1\n",
    "    elif current_node.cbreadloc > 1:\n",
    "        h_count += 3\n",
    "elif end_state[key] == value:\n",
    "    h_count += 2\n",
    "```\n",
    "\n",
    "In action, this algorithm takes way less loop iterations than the BFS.\n",
    "I tested different starting states and here are some numbers:  \n",
    "BFS - A*  \n",
    "2165 - 7  \n",
    "221 - 4  \n",
    "8645 - 52 (here BFS found a sequence that takes 23 time units and A* found one with 15)  \n",
    "\n",
    "To show some self reflection, I think that the heurisic and the calculation of the total cost are probably not optimal. Furthmore, I tested different values for the heuristic and found that the current ones work the best. Nevertheless, I am happy that I was able to implement the A* algorithm on my own and solved the problems I faced while doing so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    \n",
    "    def __init__(self, parent=None, state=None, action=None, cswitch=0, cbreadloc=0):\n",
    "        self.parent = parent\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.cswitch = cswitch\n",
    "        self.cbreadloc = cbreadloc\n",
    "\n",
    "        self.g = 0 #distance between current node and start node\n",
    "        self.h = 0 #heuristic, estimated distance from the current node to the end node\n",
    "        self.f = 0 #total node cost\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.state == other.state\n",
    "\n",
    "def calculate_h(current_node, end_node):\n",
    "    current_state = current_node.state\n",
    "    end_state = end_node.state\n",
    "    h_count = 0\n",
    "\n",
    "    for key, value in current_state.items():\n",
    "        if key == \"time\":\n",
    "            continue\n",
    "    \n",
    "        if key == \"toaster_is_on\":\n",
    "            if current_node.cswitch == 1:\n",
    "                h_count += 1\n",
    "            elif current_node.cswitch > 1:\n",
    "                h_count += 9\n",
    "        elif key == \"bread_location\":\n",
    "            if current_node.cbreadloc == 1:\n",
    "                h_count += 1\n",
    "            elif current_node.cbreadloc > 1:\n",
    "                h_count += 3\n",
    "        elif end_state[key] == value:\n",
    "            h_count += 2\n",
    "\n",
    "    return h_count\n",
    "\n",
    "\n",
    "def astar(start):\n",
    "\n",
    "    end = {'toaster_has_power': True, 'toaster_is_on': False, 'bread_location': 'plate', 'bread_state': 'toasted'} #time is not important\n",
    "\n",
    "    start_node = Node(None, start, None)\n",
    "    start_node.g = start_node.h = start_node.f = 0\n",
    "    end_node = Node(None, end, None)\n",
    "    end_node.g = end_node.h = end_node.f = 0\n",
    "\n",
    "    open_list = []\n",
    "    closed_list = []\n",
    "\n",
    "    open_list.append(start_node)\n",
    "\n",
    "    loop_counter = 0\n",
    "    while(open_list):\n",
    "        loop_counter += 1\n",
    "\n",
    "        current_node = open_list[0]\n",
    "        current_index = 0\n",
    "        for index, item in enumerate(open_list):\n",
    "            if item.f < current_node.f:\n",
    "                current_node = item\n",
    "                current_index = index\n",
    "\n",
    "        # print(current_node.state)\n",
    "        # print(current_node.f)\n",
    "\n",
    "        open_list.pop(current_index)\n",
    "        closed_list.append(current_node)\n",
    "\n",
    "        if goal(current_node.state):\n",
    "            path = []\n",
    "            current = current_node\n",
    "            print(\"ASTAR needed \" + str(loop_counter) + \" rounds.\")\n",
    "            while current.action is not None:\n",
    "                path.append(current.action)\n",
    "                current = current.parent\n",
    "            return path[::-1]\n",
    "\n",
    "        children = []\n",
    "\n",
    "        for action in actions:\n",
    "            node_state = state_transition(current_node.state, action)\n",
    "        \n",
    "            cbreadloc = current_node.cbreadloc\n",
    "            cswitch = current_node.cswitch\n",
    "\n",
    "            #testing and keeping track if actions that need multiple times (false, true, false) \n",
    "            if action == \"put_in_bread\" and current_node.state[\"bread_location\"] == \"plate\" and node_state[\"bread_location\"] == \"toaster\":\n",
    "                cbreadloc += 1\n",
    "\n",
    "            if action == \"take_out_bread\" and current_node.state[\"bread_location\"] == \"toaster\" and current_node.state[\"toaster_is_on\"] == False and current_node.state[\"bread_state\"] == \"toasted\":\n",
    "                cbreadloc += 2\n",
    "\n",
    "            if action == \"switch_on_toaster\" and node_state[\"toaster_is_on\"] == True and current_node.state[\"toaster_is_on\"] == False:\n",
    "                cswitch += 1\n",
    "\n",
    "            if action == \"wait\" and current_node.state[\"toaster_has_power\"] == True and current_node.state[\"toaster_is_on\"] == True:\n",
    "                cswitch += 2\n",
    "\n",
    "            # Create new node\n",
    "            new_node = Node(current_node, node_state, action, cswitch, cbreadloc)\n",
    "\n",
    "            # Append\n",
    "            children.append(new_node)\n",
    "\n",
    "        # Loop through children\n",
    "        for child in children:\n",
    "            \n",
    "            # Child is on the closed list\n",
    "            if child in closed_list:\n",
    "                continue\n",
    "\n",
    "            # Create the f, g, and h values\n",
    "            child.g = child.state[\"time\"]\n",
    "            child.h = calculate_h(child, end_node)\n",
    "            child.f = child.g - child.h\n",
    "\n",
    "            # print(loop_counter)\n",
    "            # print(child.action)\n",
    "            # print(child.state)\n",
    "            # print(child.f)\n",
    "            # print(\" \")\n",
    "\n",
    "            # Add the child to the open list\n",
    "            open_list.append(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Nearest Neighbour Search Algorithm\n",
    "*(This was written after the data structure explanation, please read this after the kd-tree paragraph in the data structures chapter)*\n",
    "\n",
    "One of the most important operation on the kd-tree is the nearest neighbour search. This algorithm aims to find the point in the tree that is nearest to a given input point.\n",
    "\n",
    "- using the example kd-tree from the data structure paragraph:\n",
    "    - starting at the root node, as the depth is 0, the x-axis is the split dimension\n",
    "        - the input point x-value would be compared with the root x-value and either go left or right depending on if the value is greater or lesser than the root x-value\n",
    "        - this will be done for every depth with the change of the split dimension (x-axis, y-axis, x-axis ...)\n",
    "        - the current node will become current best if the node is nearer than the current node\n",
    "        - the algorithm checks if there are nodes in the circle (the radius is the distance between the input point and the current best node and the input point is the middle point)\n",
    "            - this has to be done because points can be in the other half-space but be nearer\n",
    "            - in the code example below, this is not done by a circle but by comparing the distance between the current best point and the input point with the distance between the input point and the cutting axis\n",
    "                - if the distance of the current best and the input point is greater, the other plane could contain points that are nearer, the algorithm is then executed for the other half-space\n",
    "        - the algorithm terminates once the current best node becomes the best node (no better or new nodes)  \n",
    "        \n",
    "            \n",
    "- a real life example:\n",
    "    - If you ask google maps for, e.g., police stations. The algorithm can quickly find you the nearest police station to your location.  \n",
    "    \n",
    "\n",
    "- issues:\n",
    "    - if data set contains only a small number of points (n<100) other search algorithms can be more efficient (e.g. linear search)\n",
    "    - as stated in the data structure paragraph, high-dimensional kd-trees should be avoided and this means that nearest neighbor search gets progressively harder as the dimensionality increases\n",
    "    - static or dynamic data set\n",
    "        - either the kd-tree can be build every search if the data set has only occasional insertions\n",
    "        - if they are frequent, the kd-tree should support insertions and deletions\n",
    "        \n",
    "[This](https://www.youtube.com/channel/UCEbYhDd6c6vngsF5PQpFVWg) YouTube channel helped me a lot to understand the theory and code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 7)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a kd-tree\n",
    "def build_kdtree(points, depth=0):\n",
    "    \n",
    "    if not points:\n",
    "        return None\n",
    "    \n",
    "    k = len(points[0]) # assume all points have the same dimension\n",
    "\n",
    "    axis = depth % k # explained in data structure paragraph\n",
    "\n",
    "    sorted_points = sorted(points, key=lambda point: point[axis])\n",
    "\n",
    "    median = len(points) // 2\n",
    "    \n",
    "    return {\n",
    "        'point': sorted_points[median],\n",
    "        'left': build_kdtree(sorted_points[:median], depth + 1),\n",
    "        'right': build_kdtree(sorted_points[median+1:], depth + 1)\n",
    "    }\n",
    "\n",
    "\n",
    "# nearest neighbour search algorithm\n",
    "def distance_squared(point1, point2):\n",
    "    # assume 2-dimensional points\n",
    "    x1, y1 = point1\n",
    "    x2, y2 = point2\n",
    "\n",
    "    dx = x1 - x2\n",
    "    dy = y1 - y2\n",
    "\n",
    "    return dx * dx + dy * dy\n",
    "\n",
    "def closest_point(all_points, new_point):\n",
    "    best_point = None\n",
    "    best_distance = None\n",
    "\n",
    "    for current_point in all_points:\n",
    "        current_distance = distance_squared(new_point, current_point)\n",
    "\n",
    "        if best_distance is None or current_distance < best_distance:\n",
    "            best_distance = current_distance\n",
    "            best_point = current_point\n",
    "\n",
    "    return best_point\n",
    "\n",
    "def closer_distance(pivot, p1, p2):\n",
    "    if p1 is None:\n",
    "        return p2\n",
    "\n",
    "    if p2 is None:\n",
    "        return p1\n",
    "\n",
    "    d1 = distance_squared(pivot, p1)\n",
    "    d2 = distance_squared(pivot, p2)\n",
    "\n",
    "    if d1 < d2:\n",
    "        return p1\n",
    "    else:\n",
    "        return p2\n",
    "\n",
    "def kdtree_closest_point(root, point, depth=0):\n",
    "    if root is None:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    k = len(root[\"point\"])\n",
    "    \n",
    "    axis = depth % k\n",
    "\n",
    "    next_branch = None\n",
    "    opposite_branch = None\n",
    "\n",
    "    \n",
    "    # checking for the current axis if the point has a greater or lesser \"coordinate\" value\n",
    "    # then setting the next_branch and opposite_branch to the subtree of the root node\n",
    "    \n",
    "    if point[axis] < root['point'][axis]:\n",
    "        next_branch = root['left']\n",
    "        opposite_branch = root['right']\n",
    "    else:\n",
    "        next_branch = root['right']\n",
    "        opposite_branch = root['left']\n",
    "    \n",
    "    \n",
    "    # traverse the tree for the best point and check (closer_distance) if the new point has a closer distance to the pivot than the root point of the (sub)tree\n",
    "    best = closer_distance(point, kdtree_closest_point(next_branch, point, depth + 1), root['point'])\n",
    "\n",
    "    # this checks for a better (nearer) point on the opposite branch (as described in the introduction)\n",
    "    # it does this by comparing the distance of the current best and the point with the distance between the point and the splitting plane\n",
    "    if distance_squared(point, best) > (point[axis] - root['point'][axis]) ** 2:\n",
    "        best = closer_distance(point,\n",
    "                               kdtree_closest_point(opposite_branch,\n",
    "                                                    point,\n",
    "                                                    depth + 1),\n",
    "                               best)\n",
    "\n",
    "    return best\n",
    "\n",
    "\n",
    "\n",
    "data_set = [(2,3), (5,4), (9,6), (4,7), (8,1), (7,2)]\n",
    "\n",
    "kd_tree = build_kdtree(data_set)\n",
    "kdtree_closest_point(kd_tree, (5,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time complexity of kd-tree construction:\n",
    "- the kd-tree depth is $O(logn)$\n",
    "- sorting and finding the median takes a certain amount of time depending on the chosen sorting algorithm\n",
    "    - $O(nlogn)$ if Heapsort or Mergesort are used\n",
    "    - in our case, the sorted() function takes $O(nlogn)$ time (Timsort)\n",
    "- this happens at every level\n",
    "- thus, the time complexity of the construction in the example above is $O(nlog^2n)$\n",
    "    \n",
    "Time complexity of nearest neighbour search using kd-trees:\n",
    "- as we calculated above the depth of the kd-tree is $O(logn)$\n",
    "- in the average case kNN traverses through the levels of the tree and therefore has a time complexity of $O(logn)$\n",
    "- in the worst case, there are a maximum amount of backtracking and traversing which means a worst case time complexity of $O(n)$  \n",
    "\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data structures are structures that save and organize data. Different data structures have different attributes and using the right one makes data handling more efficient. Besides the data they hold, each data structure has certain operations that can be performed.\n",
    "Data structures are essential to every program. It is important to know different data structures and their theory to choose the right one(s) in order to have an efficient application.\n",
    "\n",
    "I tried not to use code examples since the theory is language independent and I tried to understand data structures in that independent way.\n",
    "Furthermore, there are a lot of code examples for different languages in the internet and in the books I read.\n",
    "\n",
    "> However, in practice, it is more important to avoid using a bad data structure than to identify the single best option available.\n",
    "> - Skiena, Steven S. 1998. The Algorithm Design Manual (page 367)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Linked List\n",
    "- a linear data structure\n",
    "- dynamic size\n",
    "- collection of items not stored at contiguous memory locations\n",
    "- each item stores the address (pointer) of the next item in the list\n",
    "- \"like a treasure hunt\"\n",
    "- homogeneous or heterogeneous data\n",
    "- problem: give me the last item (has to go to all the nodes to find the last one)  \n",
    "\n",
    "- reading time: $O(n)$\n",
    "- insertion/deletion time: $O(1)$\n",
    "    - changing the address of previous and following element  \n",
    "    \n",
    "- in contrast to arrays, there is only an overflow if the memory is full\n",
    "- disadvantage:\n",
    "    - require extra space for storing pointer fields\n",
    "    - can only do sequential access\n",
    "    - arrays allow better memory locality and chache performance than random pointer jumping\n",
    "    \n",
    "- doubly linked lists\n",
    "    - contain an extra pointer to the previous element\n",
    "    - thus, can be traversed in both directions\n",
    "    - insertion before a given node is more efficient\n",
    "    - disadvantage\n",
    "        - extra space for the pointer\n",
    "        - extra modification of a pointer when operations are done\n",
    "\n",
    "### 3.2 Array\n",
    "- linear data structure\n",
    "- collection of items stored at contiguous memory locations (fixed size)\n",
    "- homogeneous data\n",
    "- indexing is possible  \n",
    "\n",
    "- reading time: $O(1)$\n",
    "- insertion/deletion time: $O(n)$\n",
    "    - shifting all other elements in memory  \n",
    "    \n",
    "- they are space efficient\n",
    "    - consist purely of data (no links or other formatting information)\n",
    "- the size of an array cannot be adjusted in the middle of a program's execution\n",
    "    - the solution is a dynamic array\n",
    "        - doubling the size of the array from $m$ to $2m$ each time it runs out of space\n",
    "        - copying the contents from the old array to the new one\n",
    "- sequential and random access possible\n",
    "\n",
    "### 3.3 Stack\n",
    "- linear data structure\n",
    "- insert to the top of the stack (push)\n",
    "- read and take it from the top of the stack (pop)\n",
    "- last in, first out or first in, last out\n",
    "- push, pop time: $O(1)$ (best case implementation)  \n",
    "\n",
    "- example:\n",
    "    - recursive function call:\n",
    "        - first function is called\n",
    "            - stack: memoryfunction1\n",
    "        - first function calls second function\n",
    "            - stack: memoryfunction1, memoryfunction2\n",
    "        - second function returns\n",
    "            - stack: memoryfunction1\n",
    "    - redo-undo features (Photoshop, Editors)  \n",
    "    \n",
    "- implementation:\n",
    "    - using arrays\n",
    "    - using linked lists\n",
    "            \n",
    "- the stack, used to save the variables for multiple functions, is called the call stack (e.g. recursion)\n",
    "\n",
    "### 3.4 Queue\n",
    "- linear data structure\n",
    "- enqueue at the rear and dequeue at the front\n",
    "- first in, first out\n",
    "- enqueueing, dequeueing time: $O(1)$ (best case implementation)  \n",
    "\n",
    "- example\n",
    "    - BFS queue\n",
    "    - asynchronously transferred data  \n",
    "    \n",
    "- implementation:\n",
    "    - using arrays\n",
    "    - using linked lists\n",
    "\n",
    "\n",
    "### 3.5 Trees\n",
    "- hierarchical data structure\n",
    "- topmost node is the root\n",
    "- elements with no children are called leaves\n",
    "- trees do not have an upper limit on number of nodes as nodes are linked using pointers\n",
    "- a tree whose elements have at most 2 children is called a binary tree\n",
    "- example\n",
    "    - folder structure of hard drives\n",
    "        \n",
    "#### Binary Search Trees\n",
    "- node-based binary tree data structure\n",
    "    - left subtree of a node contains only nodes with keys lesser than the node's key\n",
    "    - right subtree of a node contains only nodes with keys greater than the node's key\n",
    "    - left and right subtree each must be a bionary search tree\n",
    "- each node contains:\n",
    "    - data\n",
    "    - pointer to left child\n",
    "    - pointer to right child  \n",
    "    \n",
    "- time complexity (worst case, h denotes the height of the tree):\n",
    "    - searching $O(h)$\n",
    "    - insertion $O(h)$\n",
    "    - deletion  $O(h)$\n",
    "    - if the tree is perfectly balanced $h = logn$\n",
    "    - if the tree has items that have been insorted in sorted order $h = n$\n",
    "\n",
    "<img src=\"https://www.geeksforgeeks.org/wp-content/uploads/binary-tree-to-DLL.png\">\n",
    "<p style=\"text-align: center;\"><a href=\"https://www.geeksforgeeks.org/wp-content/uploads/binary-tree-to-DLL.png\">Source</a></p>\n",
    "\n",
    "#### Self-Balancing Binary Search Trees\n",
    "- automatically keeps the height small\n",
    "    - adjust the tree after every insertion/deletion, so the maximum height is logarithmic ($log_2n$)\n",
    "- example implementations \n",
    "    - red-black tree\n",
    "    - splay tree\n",
    "\n",
    "#### Kd-Trees\n",
    "- a set $S$ of $n$ points or more complicated geometric objects in $k$ dimensions\n",
    "- solved problem: construct a tree that partitions space by half-planes such that each object is contained in its own box-shaped region\n",
    "<img style=\"width: 500px;\" src=\"images/kd-tree.png\">\n",
    "<p style=\"text-align: center;\">The Algorithm Design Manual, page 389</p>  \n",
    "\n",
    "- this decomposing of space into smaller number of cells provides a fast way to access any object by position  \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- example for better understanding (assume dataset of 8 points):\n",
    "<img style=\"width: 300px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/b/b6/3dtree.png\">\n",
    "<p style=\"text-align: center;\"><a href=\"https://upload.wikimedia.org/wikipedia/commons/b/b6/3dtree.png\">Source</a></p>\n",
    "- 3-dimensional kd-tree\n",
    "\n",
    "1. first partition (red plane) cuts the root cell (whole cube) into two subcells\n",
    "2. green plane than splits the subcells of the red plane\n",
    "3. blue plane does the same for the subcells of the green plane\n",
    "- we then get eight leaf cells  \n",
    " \n",
    "- each node is defined by a plane cutting through one of the dimensions\n",
    "- nodes in subsets are partitioned again\n",
    "- this process stops after $lgn$ levels, with each point in its own leaf cell\n",
    "    - if we have 8 points, we have partitioned all points after $lg8 = 3$ level\n",
    "        - as we can see in the example above: 1. red plane 2. green plane 3. blue plane  \n",
    "        \n",
    "- this example also shows that every box-shaped region is defined by $2k$ planes\n",
    "    - here we have 6 planes for each leaf cell which is true because every leaf cell is a cube\n",
    "    - if we would have a 2-dimension kd-tree we would have 4 planes for each leaf cell which also is true because a 2d cube is a rectangle and that is defined by 4 lines (one-dimensional planes)  \n",
    "\n",
    "another more concrete example:\n",
    "- assume the data set (2,3), (5,4), (9,6), (4,7), (8,1), (7,2)\n",
    "- to have a balanced tree it is efficient to select a root of the subtree that is (near) the median value of the axis selected\n",
    "    - this can be done by using a sort algorithm to sort all points with respect to one axis and then get the median point of that sorted array\n",
    "    - in this case: the point (7,2)\n",
    "        - this means that the one-dimensional plane through the x-axis would cut the space in half\n",
    "    - this will be recursively done with all subtrees until all points have been inserted in the kd-tree\n",
    "    - the only difference is that the axis will change for every depth addition\n",
    "        - this means that for the first node (depth 0) the x-axis will be the selected axis\n",
    "        - for the second the y-axis\n",
    "        - for the third the x-axis again because this is a 2-dimensional kd-tree\n",
    "            - if this would be a 3-dimensional kd-tree it would be the z-axis\n",
    "    \t- this can easily calculated by the formula ```axis = depth % dimension_of_points```\n",
    "            - depth refers to the tree depth\n",
    "                - the root is at depth 0\n",
    "                - children of the root are at depth 1\n",
    "- the resulting kd-tree will look like this\n",
    "<img style=\"width: 500px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/2/25/Tree_0001.svg\">\n",
    "<p style=\"text-align: center;\"><a href=\"https://upload.wikimedia.org/wikipedia/commons/2/25/Tree_0001.svg\">Source</a></p>\n",
    "\n",
    "<img style=\"width: 500px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/b/bf/Kdtree_2d.svg\">\n",
    "<p style=\"text-align: center;\"><a href=\"https://upload.wikimedia.org/wikipedia/commons/b/bf/Kdtree_2d.svg\">Source</a></p>  \n",
    "\n",
    "- the code implementation will be used in the \"nearest neighbour search algorithm\" paragraph  \n",
    "\n",
    "<br>\n",
    "\n",
    "- kd-trees differ in exactly how the splitting plane is selected\n",
    "- options include:\n",
    "    - cycling through the dimensions\n",
    "        - as shown in the example (x1 then x2 then x3 ...)\n",
    "    - cutting along the largest dimension\n",
    "    - quadtrees or octtrees\n",
    "    - BSP-trees\n",
    "    - R-trees\n",
    "- high-dimensional spaces should be avoided\n",
    "    - according to Steven S. Skiena, 2 up to ~20 are most useful  \n",
    "\n",
    "### 3.6 Priority Queue\n",
    "- set of records with numerically or otherwise totally-ordered keys\n",
    "- solved problem: build and maintain a data structure for providing quick access to the smallest or largest key in the set\n",
    "- each element of this data structure has a priority\n",
    "- the basic priority queue supports three primary operations\n",
    "    - insertion\n",
    "    - find-maximum or minimum\n",
    "    - delete-maximum or minimum  \n",
    "    \n",
    "- example of a priority queue implemented with an\n",
    "- unsorted array, sorted array, balanced binary search tree\n",
    "    - the sorted array is in reverse order to have the minimum element as the tail element and to not move all elements when deleting or inserting the minimum\n",
    "\n",
    "--- | unsorted array | sorted array | balanced binary search tree\n",
    "--- | --- | --- | --- |\n",
    "insert | O(1) | O(n) | O(logn)\n",
    "find-minimum | O(1) | O(1) | O(1)\n",
    "delete-minimum | O(n) | O(1) | O(logn)\n",
    "\n",
    "- it is possible to achieve constant find-minimum time for each data structure by using an extra variable to store a pointer/index to the minimum entry\n",
    "    - when asked to find it, we can simply return this value\n",
    "- if delete-minimum happens, all entries have to be searched to find the new minimum and thus, require linear time on an unsorted array and logarithmic time on a tree\n",
    "\n",
    "#### Heap\n",
    "- data structure for efficiently supporting the priority queue operations insert and extract-min\n",
    "- tree-based data structure in which the tree is a complete binary tree\n",
    "- all levels of the tree, except possibly the last one, are fully filled and, if the last level of the tree is not complete, the nodes of that level are filled from left to right\n",
    "\n",
    "- two types\n",
    "    - max-heap\n",
    "        - the key present at the root node must be greatest among the keys present at all of its children, this applies for all sub-trees\n",
    "    - min-heap\n",
    "        - the key present at the root node must be minimum among the keys present at all of its children, this applies for all sub-trees\n",
    "        \n",
    "- implementation\n",
    "    - store each key in a node with pointers to its two children\n",
    "        - memory heavy due to the pointers\n",
    "    - store data as an array of keys\n",
    "        - root of the tree at the first position\n",
    "        - left and right children at the second and third position\n",
    "        - this will store the $2^l$ keys of the $l$th level of the complete binary tree from left-to-right in positions $2^{l-1}$ to $2^l-1$\n",
    "        - these formulas assume a starting index of 1\n",
    "        - the key at position k has its left child at position $2k$ and the right child at $2k+1$\n",
    "        - the height of the heap is $logn$\n",
    "    - this second implementation does save memory but is less flexible than using pointers\n",
    "    - the problem of this structure is that it cannot store arbitrary tree topologies without wasting large amounts of space\n",
    "        - arbitrary tree topologies have sparse trees and this means that the array of keys would only work if empty entries are filled\n",
    "    - the loss of flexibility explains why representing binary search trees is not a good idea but that it works just fine for heaps\n",
    "        \n",
    "- time complexity\n",
    "    - insertion\n",
    "        - worst case is $O(logn)$\n",
    "    - deletion\n",
    "        - worst case is $O(logn)$\n",
    "    - search\n",
    "        - worst case is $O(n)$\n",
    "    - constructing a heap takes $O(nlogn)$ time\n",
    "<img style=\"width: 500px;\" src=\"https://www.geeksforgeeks.org/wp-content/uploads/MinHeapAndMaxHeap.png\">\n",
    "<p style=\"text-align: center;\"><a href=\"https://www.geeksforgeeks.org/wp-content/uploads/MinHeapAndMaxHeap.png\">Source</a></p>\n",
    "\n",
    "\n",
    "### 3.7 Dictionaries\n",
    "- a set of records, each identified by one or more key fields\n",
    "- abstract data type\n",
    "    - implementation example: hash tables\n",
    "- solved problem: build and maintain a data structure to efficiently locate, insert, and delete the record associated with any query key\n",
    "- important questions for implementations\n",
    "    - How many items?\n",
    "    - What are the operation frequencies?\n",
    "    - What is the access pattern for keys?\n",
    "    - Should individual operations be fast or the total amount of work done over the entire program be minimized?  \n",
    "    \n",
    "*Chapter 12.1 in The Algorithm Design Manual*\n",
    "    \n",
    "#### Hash Table\n",
    "- uses a hash function\n",
    "    - consistently maps a value with a particular key (hash values)\n",
    "    - these hash values can be used to index a hash table\n",
    "        - this process is called hashing\n",
    "            - a key is converted into a small integer (hash value) using a hash function\n",
    "                - can be considered to be constant $O(1)$\n",
    "            - hash code is used to find an index (hashCode % arrSize) and the entire linked list at that index (chaining) is first searched for the presence of the key\n",
    "                - worst-case time is $O(n)$\n",
    "            - if found, its value is updated and if not, the key-value pair is stored as a new node in the linked-list\n",
    "        - if there are n entries and the size of the array is b, the value n/b is called the load factor\n",
    "            - needs to be kept low, so the number of entries at one index is less and the time complexity almost constant $O(1)$\n",
    "- hash tables usually are arrays\n",
    "- hash tables can also be a different data structure\n",
    "    - e.g. balanced binary search trees\n",
    "        - insertion, deletion, searching: $O(logn)$\n",
    "        - [Interesting explanation](https://stackoverflow.com/questions/22996474/why-implement-a-hashtable-with-a-binary-search-tree)\n",
    "        \n",
    "- example phonebook, caching (redis)\n",
    "- search time: average case $O(1)$, worst case $O(n)$\n",
    "- insert time: average case $O(1)$, worst case $O(n)$\n",
    "- delete time: average case $O(1)$, worst case $O(n)$\n",
    "\n",
    "<img style=\"width: 500px;\" src=\"https://www.geeksforgeeks.org/wp-content/uploads/HashingDataStructure-min-768x384.png\">\n",
    "<p style=\"text-align: center;\"><a href=\"https://www.geeksforgeeks.org/wp-content/uploads/HashingDataStructure-min-768x384.png\">Source</a></p>\n",
    "\n",
    "- collision (assigning a value to an array slot that has already been assigned to another value) solutions\n",
    "    - chaining\n",
    "        - make each cell of hash table point to a linked list of records that have same hash function value\n",
    "        - devotes a considerable amount of memory to pointer\n",
    "        - this can slow operations\n",
    "    - open addressing\n",
    "        - all elements are stored in the hash table itself (maintained as an array of elements)\n",
    "        - each table entry contains either a record or NIL\n",
    "        - when searching, table slots are examined one by one until the desired element is found or it is clear that the element is not in the table\n",
    "        - linear probing:\n",
    "            - linearly probe for the next slot\n",
    "            - main problem is clustering of many consecutive elements\n",
    "        - quadratic probing\n",
    "            - taking the original hash index and adding successive values of an arbitrary quadratic polynomial until an open slot is found\n",
    "        - double hashing\n",
    "            - applying a second hash function to key when a collision occurs\n",
    "    - avoiding collisions is taken care of by (good) hash functions   \n",
    "    \n",
    "- rehasing\n",
    "    - when the load factor increases to more than its pre-defined value (default is 0.75), the size of the array is increased (doubled)\n",
    "    - all values are hashed again and stored in the new double sized array\n",
    "    \n",
    "- time complexity (n are items, m number of slots in the hash table)\n",
    "    - chaining and open adressing require O(m) to initialize an m-element hash table to null elements prior to the first insertion\n",
    "    - traversing chaining: O(n+m)\n",
    "    - traversing open adressing: O(m)\n",
    "    - chaining with doubly-linked lists:\n",
    "<img style=\"width: 500px;\"  src=\"images/page90.png\">\n",
    "<p style=\"text-align: center;\">The Algorithm Design Manual, page 90</p>  \n",
    "\n",
    "### 3.8 Graphs\n",
    "- non-linear data structure\n",
    "- are made up of nodes (vertices) and edges (connecting lines)\n",
    "- used to model how different things are connected to one another (network)\n",
    "- solved problem: represent a graph using a flexible, efficient data structure\n",
    "```python\n",
    "graph = {}\n",
    "graph[\"Adam\"] =[\"Philip\", \"Florencia\"]\n",
    "graph[\"Philip\"] =[]\n",
    "```  \n",
    "\n",
    "\n",
    "flavors of graphs:\n",
    "- directed\n",
    "    - the node \"Adam\" has one-way connections to the nodes \"Philip\" and \"Florencia\"\n",
    "    - \"Philip\" has no one-way connections\n",
    "    - example: maps (one-way streets)\n",
    "- undirected\n",
    "    - \"Philip\" would have a connection to \"Adam\"\n",
    "- weighted\n",
    "    - graphs have \"costs\" assigned to every edge\n",
    "    - example: drive-time, length\n",
    "- non-simple\n",
    "    - contain self-loops\n",
    "    - multiedge (edges occurs more than once in the graph)\n",
    "- acyclic\n",
    "    - contain no cycles\n",
    "    - trees are connected, acyclic undirected graphs\n",
    "- embedded\n",
    "    - vertices and edges are assigned geometric positions (drawing of a graph)\n",
    "    - any drawing of a graph is an embedding\n",
    "        - may or may not have algorithmic significance\n",
    "- topological\n",
    "    -  the structure of a graph is completely defined by the geometry of its embedding\n",
    "- implicit\n",
    "    - not constructed and then traversed\n",
    "    - built as we use them\n",
    "    - example: the state tree of the AI task where new states (nodes) were created by applying the state transition function\n",
    "- explicit\n",
    "    - explicitly constructed\n",
    "\n",
    "<img src=\"images/graphs.png\">\n",
    "<p style=\"text-align: center;\">The Algorithm Design Manual, page 147</p>  \n",
    "\n",
    "- two most commonly used representations of a graph\n",
    "    - adjacency matrix\n",
    "        - 2D array of size VxV, where V is the number of vertices\n",
    "        - for an undirected graph always symmetric  \n",
    "        \n",
    "<img style=\"width: 300px;\" src=\"https://cdncontribute.geeksforgeeks.org/wp-content/uploads/adjacencymatrix.png\">\n",
    "<p style=\"text-align: center;\"><a href=\"https://cdncontribute.geeksforgeeks.org/wp-content/uploads/adjacencymatrix.png\">Source</a></p>\n",
    "        - 1's represent that there is an edge between both vertices\n",
    "        - 0's represent that there is no edge between both vertices\n",
    "        - if this would be a weighted graph, the values (0 and 1) would be the weight value\n",
    "        - Advantages:\n",
    "            - removing an edge takes $O(1)$ time\n",
    "            - edge testing takes $O(1)$\n",
    "        - Disadvantages:\n",
    "            - consumes more space $O(V^2)$\n",
    "            - adding a vertex takes $O(V^2)$\n",
    "    - adjacency list\n",
    "        - array of lists is used\n",
    "            - size of the array is equal to $V$\n",
    "        - an entry at array index $i$ represents the list of vertices adjacent to the $i$th vertex\n",
    "        - adjacency lists are the right data structure for most applications of graphs\n",
    "<img style=\"width: 600px;\" src=\"https://cdncontribute.geeksforgeeks.org/wp-content/uploads/listadjacency.png\">\n",
    "<p style=\"text-align: center;\"><a href=\"https://cdncontribute.geeksforgeeks.org/wp-content/uploads/listadjacency.png\">Source</a></p>\n",
    "\n",
    "- questions to ask:\n",
    "    - How big will your graph be?\n",
    "        - adjacency matrices make sense for small graphs\n",
    "    - How dense will your graph be?\n",
    "        - adjacency matrices make sense for very dense graphs\n",
    "    - Which algorithms will you be implementing?\n",
    "    - Will you be modifying the graph over the course of your application?  \n",
    "    \n",
    "*Chapter 14.4 in The Algorithm Design Manual*\n",
    "\n",
    "A thing to add is this quote:\n",
    "> Designing novel graph algorithms is very hard, so don’t\n",
    "do it. Instead, try to design graphs that enable you to use classical algorithms\n",
    "to model your problem.\n",
    "> - Skiena, Steven S. 1998. The Algorithm Design Manual (page 225)\n",
    "\n",
    "\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Investigating a technology\n",
    "\n",
    "Data is everywhere and data has to be saved. Databases solve this problem by providing a mechanism for storage and retrieval. NoSQL (“not only SQL”) databases have arisen since the early 21st century. The needs of Web 2.0 and the popularity of big data were calling for newer solutions than relational databases.\n",
    "I thought that investigating the technology might lead to interesting discovery in relation to complex data structures and their usage.\n",
    "<br>\n",
    "NoSQL databases are a huge topic and so I only focus on components where data structures and their applications are used. Furthermore, there are databases for specific use-cases. There is not one solution and some even say that it does not matter if one uses NoSQL or SQL as long as they know how to prepare their data.<br>\n",
    "\n",
    "The first kind of NoSQL database is called a document-oriented database. Documents have a JSON-like structure which, essentially, are combinations of key-value pairs.\n",
    "To relate this to our data structure part, we know that we have a data structure that can be used to save key-value pairs: A dictionary (implemented as a, e.g., hash table). Of course, this does not mean that any NoSQL database uses hash tables to save and retrieve data but this gives a nice addition to the theory of hash tables since the data structure can be used to achieve a similar result.\n",
    "A more complicated JSON structure can be achieved through nested key-value pairs. Moreover, if the value of a key-value pair is, for example, an array or other data structures, the document becomes very diverse and able to represent a lot of different data.<br>\n",
    "Another important NoSQL database type is the graph database. These databases use graph structures with nodes (vertices) and edges, as well as, properties to represent and store data. The underlying storage mechanism can be a key-value store or document-oriented approach but nevertheless, the graph data structure finds its application. In the example code of the graph section, the relationship is modeled by a hash table. This shows that a database can achieve a graph structure by using the hash table data structure. <br>\n",
    "To go deeper in the graph example, we can look at one of the biggest sectors in the world-wide-web: E-commerce. Consumption graphs are graphs that track the consumption of individual customers. This helps the companies to suggest products and predict future purchases. These graphs can be saved with the help of a graph database which shows again how important the data structures are.<br>\n",
    "<br>\n",
    "Another important feature of a database is the ability to model large hierarchical or nested data relationships. To achieve that, databases use another data structure that was introduced in the previous chapter: Trees. There are different implementation methods to model this kind of data structure. One way is to reference the parent node in the child value. In a document-like value element, this could be a field with the key “parent” that holds the ID of the parent document. But the implementation does not matter for the application since the achieved structure is a tree that can be used by the database, users, and programmers to model data.<br>\n",
    "<img src=\"https://docs.mongodb.com/manual/_images/data-model-tree.bakedsvg.svg\"> <br>\n",
    "<p style=\"text-align: center;\"><a href=\"https://docs.mongodb.com/manual/_images/data-model-tree.bakedsvg.svg\">Source</a></p>  \n",
    "\n",
    "I think that the example of databases shows the importance of data structures. What is also shows is that good knowledge of the possibilities of each is required to make rational decisions that help the program to work more efficiently, espcially if one plans to implement a database oneself. Furthermore, it also emphasizes the ability that different structures can be achieved in different ways. For that, the knowledge about different data structures, their advantages, and disadvantages is necessary for a developer that handles data (which, in theory, is every program).<br>\n",
    "\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another technology that we can investigate is an AI. In particular, the AI that plans to toast a bread by selecting the right action sequence. This was a task for the AI module and I showed the used algorithms in the algorihms chapter. Now I want to focus on the data structures that are used.  \n",
    "\n",
    "\n",
    "The first data structure that is used is a tuple. This is an immutable sequence that is typically used to store collections of heterogeneous data. There are also used if an immutable sequence of homogeneous data is needed. They can be compared with the array data structure even though they are mutable.  \n",
    "```python\n",
    "actions=(\n",
    "    \"plug_in_toaster\",\n",
    "    \"unplug_toaster\",\n",
    "    \"put_in_bread\",\n",
    "    \"take_out_bread\",\n",
    "    \"switch_on_toaster\",\n",
    "    \"wait\")\n",
    "```  \n",
    "\n",
    "Python has a another data structure that comparable to the array data structure which is called a list. This is a mutable sequence of, typically, homogeneous data.  \n",
    "In the case of the AI application it does not matter if the data structure is a tuple or a list. The only thing that is done with the data structure is a looping through all of its items. Nevertheless, as tuples are immutable and the actions do not change, a tuple is the right choice. Furthermore, tuples are more efficient than lists because they are immutable ([Source](https://stackoverflow.com/questions/68630/are-tuples-more-efficient-than-lists-in-python)).\n",
    "So it would be possible to use both, lists and tuples, but the right choice is the tuple data structure. This case shows, why it is important to know the data structures and when to use which.\n",
    "\n",
    "<br>\n",
    "\n",
    "The second data structure that is used is a Python dictionary.  \n",
    "\n",
    "```python\n",
    "state={\n",
    "    \"toaster_has_power\":False,\n",
    "    \"toaster_is_on\":False,\n",
    "    \"bread_location\":\"plate\",\n",
    "    \"bread_state\":\"untoasted\",\n",
    "    \"time\":0\n",
    "    }\n",
    "```  \n",
    "\n",
    "This data structure is internally implemented as a hash table. Python dictionaries are unordered and mutable objects. In our case we are mapping a string to a value. Strings are hashable data types and thus, the dictionary can be used to map a value to the key string. We are also only accessing the values by their keys. This means that it does not matter that the dictionary is unordered. Thus, the dictionary data structure is the right choice. If the key would be a tuple, the hashing would also work due to their immutability. Lists would, therefore, not work.\n",
    "\n",
    "<br>\n",
    "\n",
    "The last data structure I want to highlight, it the open_list and close_list of the A* algorithm.  \n",
    "\n",
    "```python\n",
    "open_list = []\n",
    "closed_list = []\n",
    "\n",
    "open_list.append(start_node)\n",
    "\n",
    "loop_counter = 0\n",
    "while(open_list):\n",
    "    loop_counter += 1\n",
    "\n",
    "    current_node = open_list[0]\n",
    "    current_index = 0\n",
    "    for index, item in enumerate(open_list):\n",
    "        if item.f < current_node.f:\n",
    "            current_node = item\n",
    "            current_index = index\n",
    "\n",
    "    # print(current_node.state)\n",
    "    # print(current_node.f)\n",
    "\n",
    "    open_list.pop(current_index)\n",
    "    closed_list.append(current_node)\n",
    "```  \n",
    "\n",
    "This code snippet clearly shows the reasoning for the chosen data structure. Both lists are Python list objects. As stated above, these are mutable sequences of, typically, homogeneous data. We can also see that we are appending nodes and popping nodes of the lists. This is not possible with a tuple. The Python set data structure could not have been used because they are unordered objects with no duplicates.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "I think that this investigation shows nicely, why data structures are important and why it makes a lot of sense to know the theory about them. Furthermore, it emphasizes that the theory in a specific language is also important in order to reason about the right data structure implementation in that language.  \n",
    "\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sources\n",
    "- Skiena, Steven S. 1998. The Algorithm Design Manual: Text. Vol. 1. Springer Science & Business Media\n",
    "- Bhargava, Aditya. 2016. Grokking Algorithms: An Illustrated Guide for Programmers and Other Curious People\n",
    "- https://www.khanacademy.org/computing/computer-science/algorithms\n",
    "- http://cslibrary.stanford.edu/103/LinkedListBasics.pdf\n",
    "- http://introcs.cs.princeton.edu/43stack/\n",
    "- https://stackoverflow.com/questions/7477181/array-based-vs-list-based-stacks-and-queues\n",
    "- http://www.cs.cornell.edu/courses/cs312/2004fa/lectures/lecture16.htm#:~:text=A%20good%20rule%20of%20thumb,*n2%2Bq\n",
    "- https://www.geeksforgeeks.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
